{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seulmi0827/fininsight/blob/main/JACE/refine_sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ME022qAbmf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ln95Khm0fkrw"
      },
      "outputs": [],
      "source": [
        "!pip install -q kss flashtext\n",
        "!pip install -q pandarallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XDwxn4tP60s7"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk -y\n",
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!apt-get install curl -y\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9trurtnXbB1l",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import pandas as pd\n",
        "import re\n",
        "from kss import split_sentences\n",
        "from flashtext import KeywordProcessor\n",
        "from pandarallel import pandarallel\n",
        "from konlpy.tag import Mecab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg6S16jGwN0Z"
      },
      "outputs": [],
      "source": [
        "def expand_aspect_terms(aspect_terms: List[str]) -> List[str]:\n",
        "    expanded = set()\n",
        "\n",
        "    for term in aspect_terms:\n",
        "        if '_' in term:\n",
        "            clean_term = term.replace('_', ' ')\n",
        "            expanded.add(clean_term) # 언더바를 띄어쓰기로 대체한 단어 추가\n",
        "            expanded.add(clean_term.replace(\" \", \"\"))  # 띄어쓰기를 제거한 단어 추가\n",
        "        elif ' ' in term:\n",
        "            expanded.add(term) # 띄어쓰기 된 원본 단어 추가\n",
        "            clean_term = term.replace(' ', '')\n",
        "            expanded.add(clean_term) # 띄어쓰기 제거된 단어 추가\n",
        "        else:\n",
        "            clean_term = term # 언더바도 아니고 띄어쓰기도 아닌 원래 단어\n",
        "            expanded.add(clean_term)\n",
        "\n",
        "    return list(expanded)\n",
        "\n",
        "\n",
        "def extract_aspect_sentences(df, aspect_terms):\n",
        "    pandarallel.initialize(progress_bar=True, verbose=1)\n",
        "\n",
        "    expanded_terms = expand_aspect_terms(aspect_terms)\n",
        "\n",
        "    keyword_processor = KeywordProcessor()\n",
        "    for term in expanded_terms:\n",
        "        keyword_processor.add_keyword(term)\n",
        "\n",
        "    # 정규식 패턴 생성\n",
        "    pattern_dict = {\n",
        "        term: re.compile(rf'\\b{re.escape(term)}\\b') for term in expanded_terms\n",
        "    }\n",
        "\n",
        "    def process_content(content):\n",
        "        results = []\n",
        "        try:\n",
        "            # KSS로 문장 분리\n",
        "            sentences = split_sentences(content)\n",
        "\n",
        "            for sentence in sentences:\n",
        "                # 조사 제거 처리\n",
        "                mecab = Mecab()\n",
        "                processed_sentence = sentence # 원본문장 복제\n",
        "\n",
        "                # 문장 형태소 분석\n",
        "                pos_tagged = mecab.pos(sentence)\n",
        "\n",
        "                # 뒤에서부터 조사 제거 (위치 변경 방지)\n",
        "                current_pos = len(sentence)\n",
        "                for i in range(len(pos_tagged)-1, -1, -1):\n",
        "                    word, pos = pos_tagged[i]\n",
        "                    if pos.startswith(\"J\"): # \"J\", \"V\", \"E\", \"M\", \"X\", \"SL\", \"SW\", \"SP\", \"SF\", \"SS\"\n",
        "                        word_pos = sentence.rfind(word, 0, current_pos)\n",
        "                        if word_pos != -1:\n",
        "                            processed_sentence = processed_sentence[:word_pos] + processed_sentence[word_pos+len(word):]\n",
        "                            current_pos = word_pos\n",
        "\n",
        "                # 키워드 검색\n",
        "                if keyword_processor.extract_keywords(processed_sentence):\n",
        "                    for term, pattern in pattern_dict.items():\n",
        "                        if pattern.search(processed_sentence):\n",
        "                            results.append({'original_sentence': sentence, 'processed_sentence': processed_sentence, 'term': term})\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "            pass\n",
        "        return results\n",
        "\n",
        "    # 병렬 처리\n",
        "    all_results = df['content'].parallel_apply(process_content)\n",
        "\n",
        "    # 결과 합치기\n",
        "    flattened_results = [item for sublist in all_results for item in sublist]\n",
        "    return pd.DataFrame(flattened_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "luD8IFNMa8uw"
      },
      "outputs": [],
      "source": [
        "# 1. Aspect terms 불러오기\n",
        "aspect_df = pd.read_csv('/content/drive/MyDrive/fin/ABSA/사회_both_1.csv')\n",
        "aspect_df_add = pd.read_csv('/content/drive/MyDrive/fin/ABSA/사회_disagreement_gpt_공유용.csv')\n",
        "aspect_terms1 = aspect_df['단어'].dropna().astype(str).tolist()\n",
        "aspect_terms2 = aspect_df_add['단어'].dropna().astype(str).tolist()\n",
        "aspect_terms = aspect_terms1 + aspect_terms2\n",
        "print(f\"로드된 aspect terms: {len(aspect_terms)}개\")\n",
        "print()\n",
        "\n",
        "# 2. 기사 데이터 불러오기\n",
        "df = pd.read_csv('/content/drive/MyDrive/fin/전남소멸_10000.csv', encoding='utf-8')\n",
        "df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n",
        "print(df['content'])\n",
        "print(f\"로드된 기사: {len(df)}개\")\n",
        "print()\n",
        "\n",
        "# 3. 병렬 처리로 문장 추출 실행\n",
        "print(\"문장 추출 시작...\")\n",
        "result_df = extract_aspect_sentences(df, aspect_terms)\n",
        "result_df\n",
        "\n",
        "# 4. 결과 저장\n",
        "result_df.to_csv('/content/drive/MyDrive/fin/0420_사회_전남소멸_aspect_sentences.csv', index=False, encoding='utf-8')\n",
        "print(f\"추출된 문장: {len(result_df)}개\")\n",
        "\n",
        "# 5. 결과 샘플 출력\n",
        "print(\"\\n결과 샘플:\")\n",
        "print(result_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7U7RmGvZnB0s"
      },
      "outputs": [],
      "source": [
        "# term별 문장 수 계산\n",
        "term_counts = result_df.groupby('term')['target_sentence'].nunique().reset_index()\n",
        "term_counts.columns = ['term', 'sentence_count']\n",
        "\n",
        "# 문장 수 기준 내림차순 정렬\n",
        "term_counts = term_counts.sort_values('sentence_count', ascending=False)\n",
        "\n",
        "# 결과 저장\n",
        "term_counts.to_csv('/content/drive/MyDrive/fin/term_sentence_counts.csv', index=False, encoding='utf-8')\n",
        "\n",
        "# 상위 10개 term 출력\n",
        "print(\"상위 10개 term:\")\n",
        "print(term_counts.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "dD19909fyKNy"
      },
      "outputs": [],
      "source": [
        "# 1. Aspect terms 불러오기\n",
        "aspect_df = pd.read_csv('/content/drive/MyDrive/fin/ABSA/사회_both_1.csv')\n",
        "aspect_df_add = pd.read_csv('/content/drive/MyDrive/fin/ABSA/사회_disagreement_gpt_공유용.csv')\n",
        "aspect_terms1 = aspect_df['단어'].dropna().astype(str).tolist()\n",
        "aspect_terms2 = aspect_df_add['단어'].dropna().astype(str).tolist()\n",
        "aspect_terms = aspect_terms1 + aspect_terms2\n",
        "print(f\"로드된 aspect terms: {len(aspect_terms)}개\")\n",
        "print()\n",
        "\n",
        "# 2. 기사 데이터 불러오기\n",
        "df = pd.read_csv('/content/drive/MyDrive/fin/경북산불.csv', encoding='utf-8')\n",
        "df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)\n",
        "print(df['content'])\n",
        "print(f\"로드된 기사: {len(df)}개\")\n",
        "print()\n",
        "\n",
        "\n",
        "# 3. 병렬 처리로 문장 추출 실행\n",
        "print(\"병렬 문장 추출 시작...\")\n",
        "result_df = extract_aspect_sentences(df, aspect_terms)\n",
        "print()\n",
        "\n",
        "# 4. 결과 저장\n",
        "result_df.to_csv('/content/drive/MyDrive/fin/0420_사회_경북산불_aspect_sentences.csv', index=False, encoding='utf-8')\n",
        "print(f\"추출된 문장: {len(result_df)}개\")\n",
        "\n",
        "# 5. 결과 샘플 출력\n",
        "print(\"\\n결과 샘플:\")\n",
        "print(result_df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X19_l0kb4sYR"
      },
      "outputs": [],
      "source": [
        "term별 문장 수 계산\n",
        "term_counts = result_df.groupby('term')['target_sentence'].nunique().reset_index()\n",
        "term_counts.columns = ['term', 'sentence_count']\n",
        "\n",
        "# 문장 수 기준 내림차순 정렬\n",
        "term_counts = term_counts.sort_values('sentence_count', ascending=False)\n",
        "\n",
        "# 결과 저장\n",
        "term_counts.to_csv('/content/drive/MyDrive/fin/term_sentence_counts.csv', index=False, encoding='utf-8')\n",
        "\n",
        "# 상위 10개 term 출력\n",
        "print(\"상위 10개 term:\")\n",
        "print(term_counts.head())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}