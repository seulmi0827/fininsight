{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seulmi0827/fininsight/blob/main/JACE/BERTopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KKg9qiHMveXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q bertopic\n",
        "!pip install -q bertopic[visualization]"
      ],
      "metadata": {
        "id": "m39j846ZIk_l",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk -y\n",
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!apt-get install curl -y\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2HV9vZ72ztpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 전처리"
      ],
      "metadata": {
        "id": "eiXsIGy7Yqyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def parse_datetime(date_string):\n",
        "   try:\n",
        "       return datetime.fromisoformat(date_string)\n",
        "   except ValueError:\n",
        "       print(f\"Invalid date format: {date_string}\")\n",
        "       return None"
      ],
      "metadata": {
        "id": "BFYSdKsuKCyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab\n",
        "import pandas as pd\n",
        "\n",
        "mecab = Mecab()\n",
        "\n",
        "def preprocess(text):\n",
        "    # 명사 추출\n",
        "    nouns = mecab.nouns(text)\n",
        "\n",
        "    # 형태소 분석\n",
        "    pos_tagged = mecab.pos(text)\n",
        "\n",
        "    # 동사 추출 (VV: 동사)\n",
        "    verbs = [word for word, pos in pos_tagged if pos.startswith('VV')]\n",
        "\n",
        "    # 형용사 추출 (VA: 형용사)\n",
        "    adjectives = [word for word, pos in pos_tagged if pos.startswith('VA')]\n",
        "\n",
        "    # 부사 추출 (MAG: 일반부사)\n",
        "    adverbs = [word for word, pos in pos_tagged if pos == 'MAG']\n",
        "\n",
        "    # 모든 단어 결합 (1글자 이상인 것만)\n",
        "    all_words = nouns + verbs + adjectives + adverbs\n",
        "    filtered_words = [word for word in all_words if len(word) > 1]\n",
        "\n",
        "    # 공백으로 조인하여 문자열로 반환\n",
        "    return filtered_words\n"
      ],
      "metadata": {
        "id": "FIKGZB7SzSw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터프레임 로드\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/fin/삼성전자_10000.csv\")\n",
        "\n",
        "df['inp_date'] = df['inp_date'].apply(parse_datetime)\n",
        "\n",
        "df['content'] = df['content'].fillna('').astype(str)\n",
        "original_content = df['content'].tolist()\n",
        "\n",
        "preprocessed_content = [preprocess(preprocessed_content) for preprocessed_content in original_content]\n",
        "df['preprocessed_content'] = [' '.join(content) for content in preprocessed_content]"
      ],
      "metadata": {
        "id": "xjA6M7xyKFNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "4TOJsNJSfhY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns.tolist()"
      ],
      "metadata": {
        "id": "hYJpxaq3H4Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['inp_date', 'preprocessed_content']].head()"
      ],
      "metadata": {
        "id": "wHIIKFZuIbO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mecab과 SBERT를 이용한 Bertopic"
      ],
      "metadata": {
        "id": "XQTO-lhg1hAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ],
      "metadata": {
        "id": "8gkrnj7A4enM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "\n",
        "# SEED = 42\n",
        "# random.seed(SEED)\n",
        "# np.random.seed(SEED)\n",
        "# torch.manual_seed(SEED)\n",
        "# if torch.cuda.is_available():\n",
        "#   torch.cuda.manual_seed_all(SEED)\n",
        "#   torch.backends.cudnn.deterministic = True\n",
        "#   torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "c6M1Fc2Ja4LY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# umap_model = UMAP(\n",
        "#    n_neighbors=15,\n",
        "#    n_components=5,\n",
        "#    min_dist=0.0,\n",
        "#    metric='cosine',\n",
        "#    random_state=SEED\n",
        "# )\n",
        "\n",
        "# hdbscan_model = HDBSCAN(\n",
        "#    min_cluster_size=10, # 1개 주제를 만드려면 최소 3개의 비슷한 문서가 있어야 한다는 말. 적은 문서에서 nr_topics의 지정값이 출력되지 않아서 이를 조정\n",
        "#    metric='euclidean',\n",
        "#    prediction_data=True, # 모델 초기화\n",
        "#    gen_min_span_tree=True,\n",
        "#    cluster_selection_method='eom'\n",
        "# )\n",
        "\n",
        "\n",
        "embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=None) # TF-IDF\n",
        "\n",
        "topic_model = BERTopic(\n",
        "   language=\"korean\",\n",
        "   nr_topics=30,\n",
        "   top_n_words=5,\n",
        "   calculate_probabilities=True,\n",
        "  #  umap_model=umap_model,\n",
        "  #  hdbscan_model=hdbscan_model,\n",
        "   embedding_model=embedding_model,\n",
        "   vectorizer_model=vectorizer, # TF-IDF to c-TF-IDF\n",
        "   verbose=True, # 로그 추적\n",
        ")\n",
        "\n",
        "\n",
        "content_for_topic = [' '.join(doc) for doc in preprocessed_content]\n",
        "topics, probs = topic_model.fit_transform(content_for_topic)\n",
        "\n",
        "\n",
        "# topics는 각 문서가 할당된 주요 토픽의 ID를 담은 배열입니다.\n",
        "# probs는 각 문서가 모든 토픽에 대해 가지는 확률값을 담은 2D 배열입니다. 크기는 (문서 수 × 토픽 수)입니다.\n",
        "\n",
        "# calculate_probabilities 옵션의 차이는 다음과 같습니다:\n",
        "  # True로 설정 시:\n",
        "  # 각 문서가 각 토픽에 속할 확률을 계산합니다\n",
        "  # fit_transform()이 토픽 ID와 확률값 두 가지를 반환합니다: topics, probs = fit_transform()\n",
        "  # 문서가 여러 토픽에 걸쳐 있는 정도를 알 수 있습니다\n",
        "  # 계산 시간이 더 오래 걸립니다\n",
        "\n",
        "  # False로 설정 시(기본값):\n",
        "  # 각 문서를 가장 가능성 높은 하나의 토픽에만 할당합니다\n",
        "  # fit_transform()이 토픽 ID만 반환합니다: topics = fit_transform()\n",
        "  # 계산이 더 빠릅니다\n",
        "  # 다중 토픽 정보는 얻을 수 없습니다"
      ],
      "metadata": {
        "collapsed": true,
        "id": "A6iIQ8Z34VyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(topic_model.get_topic_info()))"
      ],
      "metadata": {
        "id": "B6qDxnDABD_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(topic_model.get_topic_info()))"
      ],
      "metadata": {
        "id": "EhMKA4cOA7aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# index : 토픽 인덱스는 단순히 모델이 내부적으로 할당한 식별자일 뿐, 중요성이나 순위를 나타내지 않습니다.\n",
        "# Topic: 토픽 ID 번호 (-1은 아웃라이어 토픽)\n",
        "# Count: 토탈 문서 수 중, 각 토픽에 할당된 문서 수\n",
        "# Name: 토픽의 자동 생성된 이름 (주요 키워드로 구성)\n",
        "# Representation: 토픽을 대표하는 주요 단어들 (top_n_words=k에 해당)\n",
        "# Representative_Docs: 각 토픽을 가장 잘 대표하는 문서 예시, Representative_Docs에서 추출된 정보로 Representation을 만드는 게 아님!!!\n",
        "\n",
        "# Topic 아웃라이너 : BERTopic에서 토픽 ID가 -1로 표시되는 특별한 분류\n",
        "# 주요 토픽들에 잘 맞지 않는 문서들의 집합 (일관된 주제가 없거나 여러 주제가 혼합된 문서들이 포함, 아주 독특한 내용을 가진 문서들이 포함)\n",
        "# BERTopic은 HDBSCAN 클러스터링을 사용하는데, 이 알고리즘은 노이즈가 많거나 데이터 밀도가 낮은 영역의 포인트를 아웃라이어로 분류"
      ],
      "metadata": {
        "id": "dqHKu6579ylJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"총 문서 수 : \", topic_model.get_topic_info()['Count'].sum())"
      ],
      "metadata": {
        "id": "IevB-DYOB--W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_df = topic_model.get_topic_info()\n",
        "topic_df = topic_df[topic_df['Topic'] != -1][['Topic', 'Representation']]\n",
        "topic_df['Representation'] = topic_df['Representation'].apply(lambda x: ', '.join(x))\n",
        "topic_df[['Topic', 'Representation']]\n",
        "topic_df.head()"
      ],
      "metadata": {
        "id": "3NwLdC1xOAtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['topic_id'] = topics\n",
        "df[['inp_date', 'preprocessed_content', 'topic_id']].head()"
      ],
      "metadata": {
        "id": "hI4aKI7FBQHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['topic_keywords'] = df['topic_id'].map(dict(zip(topic_df['Topic'], topic_df['Representation'])))\n",
        "# Topic은 모델에서 나온 \"토픽 키워드에 대한 ID\", Representation은 \"토픽 키워드에 대한 ID\"에 해당하는 밸류(실제 토픽 키워드)\n",
        "\n",
        "df[['inp_date', 'preprocessed_content', 'topic_keywords']]"
      ],
      "metadata": {
        "id": "rVBWjApnVY45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_keyword_distribution = df.groupby([df['inp_date'].dt.date, 'topic_keywords']).size().unstack(fill_value=0)\n",
        "\n",
        "\n",
        "import plotly.graph_objs as go\n",
        "\n",
        "traces = []\n",
        "for column in topic_keyword_distribution.columns:\n",
        "    trace = go.Scatter(\n",
        "        x=topic_keyword_distribution.index,\n",
        "        y=topic_keyword_distribution[column],\n",
        "        mode='lines+markers',\n",
        "        name=column\n",
        "    )\n",
        "    traces.append(trace)\n",
        "\n",
        "layout = go.Layout(\n",
        "    title='상승 키워드 분석',\n",
        "    xaxis={'title': '날짜'},\n",
        "    yaxis={'title': '문서 수'}\n",
        ")\n",
        "\n",
        "fig = go.Figure(data=traces, layout=layout)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "ycw2g6lbSRGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barchart = topic_model.visualize_barchart()\n",
        "barchart.show()\n",
        "print('\\n')\n",
        "\n",
        "heatmap = topic_model.visualize_heatmap()\n",
        "heatmap.show()\n",
        "print('\\n')\n",
        "\n",
        "hierarchy = topic_model.visualize_hierarchy()\n",
        "hierarchy.show()\n",
        "print('\\n')\n",
        "\n",
        "term_rank = topic_model.visualize_term_rank()\n",
        "term_rank.show()\n",
        "print('\\n')\n",
        "\n",
        "topics = topic_model.visualize_topics()\n",
        "topics.show()"
      ],
      "metadata": {
        "id": "rrnLyF3JDJlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N9OkiNizWLR9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}