{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seulmi0827/fininsight/blob/main/JACE/BERTopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KKg9qiHMveXi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "m39j846ZIk_l"
      },
      "outputs": [],
      "source": [
        "!pip install -q bertopic\n",
        "!pip install -q bertopic[visualization]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2HV9vZ72ztpC"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk -y\n",
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!apt-get install curl -y\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiXsIGy7Yqyf"
      },
      "source": [
        "# ì „ì²˜ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjA6M7xyKFNo"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def parse_datetime(date_string):\n",
        "   try:\n",
        "       return datetime.fromisoformat(date_string)\n",
        "   except ValueError:\n",
        "       print(f\"Invalid date format: {date_string}\")\n",
        "       return None\n",
        "\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "import pandas as pd\n",
        "\n",
        "mecab = Mecab()\n",
        "\n",
        "def preprocess(text):\n",
        "    pos_tagged = mecab.pos(text)\n",
        "\n",
        "    filtered = [\n",
        "        word for word, pos in pos_tagged\n",
        "        if (\n",
        "            pos.startswith('NN') or  # ëª…ì‚¬\n",
        "            pos.startswith('VV') or  # ë™ì‚¬\n",
        "            pos.startswith('VA') or  # í˜•ìš©ì‚¬\n",
        "            pos == 'MAG'             # ì¼ë°˜ ë¶€ì‚¬\n",
        "        ) and len(word) > 1          # 1ê¸€ì ì´ìƒë§Œ\n",
        "    ]\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    # ë°ì´í„°í”„ë ˆì„ ë¡œë“œ\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # ì£¼ë§ ë‰´ìŠ¤ ë¹„ìœ¨ í™•ì¸ (ì°¸ê³ ìš©)\n",
        "    test = pd.read_csv(file_path)\n",
        "    test['inp_date'] = pd.to_datetime(test['inp_date'])\n",
        "    test['weekday'] = test['inp_date'].dt.weekday\n",
        "\n",
        "    # ë‚ ì§œ ë²”ìœ„ ê³„ì‚°\n",
        "    min_date = test['inp_date'].min()\n",
        "    max_date = test['inp_date'].max()\n",
        "    total_days = (max_date - min_date).days + 1\n",
        "\n",
        "    # ì£¼ë§ ë‚ ì§œ ìˆ˜ ê³„ì‚°\n",
        "    date_range = pd.date_range(start=min_date, end=max_date)\n",
        "    weekend_days = sum(date.weekday() >= 5 for date in date_range)\n",
        "    weekday_days = total_days - weekend_days\n",
        "\n",
        "    print(f\"ë°ì´í„° ê¸°ê°„: {min_date} ~ {max_date}\")\n",
        "    print(f\"ì´ ê¸°ê°„: {total_days}ì¼\")\n",
        "    print(f\"í‰ì¼ ê¸°ê°„: {weekday_days}ì¼\")\n",
        "    print(f\"ì£¼ë§ ê¸°ê°„: {weekend_days}ì¼\")\n",
        "\n",
        "    # ì£¼ë§ ë‚ ì§œ í™•ì¸\n",
        "    print(\"ì£¼ë§ ë‚ ì§œ í™•ì¸ : \")\n",
        "    print(test[test['weekday'].isin([5, 6])][['inp_date', 'weekday']])\n",
        "    print()\n",
        "\n",
        "    # í†µê³„ ì¶œë ¥\n",
        "    total_news = len(test)\n",
        "    weekend = len(test[test['weekday'].isin([5, 6])])\n",
        "    weekday = total_news - weekend\n",
        "\n",
        "    print(f\"í‰ì¼ ê¸°ì‚¬ ìˆ˜: {weekday}ê°œ ({weekday / total_news * 100:.2f}%)\")\n",
        "    print(f\"ì£¼ë§ ê¸°ì‚¬ ìˆ˜: {weekend}ê°œ ({weekend / total_news * 100:.2f}%)\")\n",
        "\n",
        "    # ë³¸ ë°ì´í„° ì „ì²˜ë¦¬\n",
        "    df['inp_date'] = df['inp_date'].apply(parse_datetime)\n",
        "    df = df[~df['inp_date'].dt.dayofweek.isin([5, 6])]\n",
        "    print('ì£¼ë§ ì œê±°ëœ ë¬¸ì„œ ìˆ˜ : ', len(df))\n",
        "\n",
        "    # ì£¼ë§ ì œê±° í›„ ë‚ ì§œë³„ ë°ì´í„° í™•ì¸\n",
        "    df['date'] = df['inp_date'].dt.date  # ì‹œê°„ ì •ë³´ ì œì™¸í•œ ë‚ ì§œë§Œ ì¶”ì¶œ\n",
        "    date_counts = df['date'].value_counts().sort_index()\n",
        "\n",
        "    # ì „ì²´ ê¸°ê°„ ìƒì„±\n",
        "    full_date_range = pd.date_range(start=df['inp_date'].min().date(),\n",
        "                                   end=df['inp_date'].max().date(),\n",
        "                                   freq='D')\n",
        "\n",
        "    # ì£¼ë§ ì œì™¸í•œ í‰ì¼ë§Œ í•„í„°ë§\n",
        "    weekday_dates = [date for date in full_date_range if date.weekday() < 5]\n",
        "\n",
        "    # ë‚ ì§œë³„ ë¬¸ì„œ ìˆ˜ í†µê³„\n",
        "    print(\"\\n== ë‚ ì§œë³„ ë¬¸ì„œ ìˆ˜ í†µê³„ ==\")\n",
        "    print(f\"í‰ê·  ë¬¸ì„œ ìˆ˜: {date_counts.mean():.2f}\")\n",
        "    print(f\"ìµœì†Œ ë¬¸ì„œ ìˆ˜: {date_counts.min()} (ë‚ ì§œ: {date_counts.idxmin()})\")\n",
        "    print(f\"ìµœëŒ€ ë¬¸ì„œ ìˆ˜: {date_counts.max()} (ë‚ ì§œ: {date_counts.idxmax()})\")\n",
        "\n",
        "    # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬\n",
        "    df['content'] = df['content'].fillna('').astype(str)\n",
        "    df['preprocessed_content'] = df['content'].apply(lambda x: ' '.join(preprocess(x)))\n",
        "\n",
        "    # í† í”½ ëª¨ë¸ë§ìš© ë°ì´í„° ì¤€ë¹„\n",
        "    preprocessed_content = df['content'].apply(preprocess).tolist()\n",
        "\n",
        "    return df, preprocessed_content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df, preprocessed_content = load_and_preprocess_data(\"/content/drive/MyDrive/Colab Notebooks/á„‰á…¡á†·á„‰á…¥á†¼á„Œá…¥á†«á„Œá…¡_10000.csv\")"
      ],
      "metadata": {
        "id": "d2BfXQ0ofL9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYJpxaq3H4Fr"
      },
      "outputs": [],
      "source": [
        "print('ìƒˆë¡œ ì¶”ê°€ëœ ì „ì²˜ë¦¬ ì»¬ëŸ¼ í™•ì¸ : ', df.columns.tolist())\n",
        "print()\n",
        "print(df[['inp_date', 'preprocessed_content']][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQTO-lhg1hAB"
      },
      "source": [
        "# Mecabê³¼ SBERTë¥¼ ì´ìš©í•œ Bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gkrnj7A4enM"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def initialize_topic_model(seed):\n",
        "    umap_model = UMAP(\n",
        "        n_neighbors=15,\n",
        "        n_components=5,\n",
        "        min_dist=0.0,\n",
        "        metric='cosine',\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    hdbscan_model = HDBSCAN(\n",
        "        min_cluster_size=20,\n",
        "        metric='euclidean',\n",
        "        prediction_data=True,\n",
        "        gen_min_span_tree=True,\n",
        "        cluster_selection_method='eom'\n",
        "    )\n",
        "\n",
        "    embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "    vectorizer = CountVectorizer(stop_words=None)\n",
        "\n",
        "    return BERTopic(\n",
        "        language=\"korean\",\n",
        "        nr_topics=20, # ìœ íš¨ í† í”½ ê²°ì •\n",
        "        top_n_words=7, # í† í”½ ë³„ í‚¤ì›Œë“œ ìˆ˜ ê²°ì •\n",
        "        calculate_probabilities=True,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        embedding_model=embedding_model,\n",
        "        vectorizer_model=vectorizer,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "def run_topic_modeling(preprocessed_content):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"ì‚¬ìš©ì¤‘ì¸ ë””ë°”ì´ìŠ¤ : \", device)\n",
        "    print()\n",
        "\n",
        "    # SEED = 42\n",
        "    # set_seed(SEED)\n",
        "    random_seed = random.randint(0, 10000)\n",
        "\n",
        "    topic_model = initialize_topic_model(random_seed)\n",
        "    content_for_topic = [' '.join(doc) for doc in preprocessed_content]\n",
        "    topics, probs = topic_model.fit_transform(content_for_topic)\n",
        "\n",
        "    return topics, probs, topic_model"
      ],
      "metadata": {
        "id": "VCT4N5u3VtQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs, topic_model = run_topic_modeling(preprocessed_content)"
      ],
      "metadata": {
        "id": "om5rZFc0VwNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_topic_distribution(topic_model, topics):\n",
        "    # í† í”½ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "\n",
        "    # ìœ íš¨ í† í”½ê³¼ ë…¸ì´ì¦ˆ ë¶„ë¦¬\n",
        "    valid_topics = topic_info[topic_info['Topic'] != -1]\n",
        "    noise_info = topic_info[topic_info['Topic'] == -1]\n",
        "\n",
        "    # í† í”½ í†µê³„ ê³„ì‚°\n",
        "    num_topics = valid_topics.shape[0]\n",
        "    total_docs = len(topics)\n",
        "    noise_count = noise_info['Count'].values[0] if not noise_info.empty else 0\n",
        "    valid_docs = total_docs - noise_count\n",
        "\n",
        "    # ê²°ê³¼ ì¶œë ¥\n",
        "    print(f\"=== í† í”½ ë¶„ì„ ê²°ê³¼ ===\")\n",
        "    print(f\"ìœ íš¨ í† í”½ ìˆ˜: {num_topics}ê°œ\")\n",
        "    print(f\"ì „ì²´ ë¬¸ì„œ ìˆ˜: {total_docs}ê°œ\")\n",
        "    print(f\"ë…¸ì´ì¦ˆ ë¬¸ì„œ ìˆ˜: {noise_count}ê°œ ({noise_count/total_docs:.2%})\")\n",
        "    print(f\"ìœ íš¨ í† í”½ ë¬¸ì„œ ìˆ˜: {valid_docs}ê°œ ({valid_docs/total_docs:.2%})\")\n",
        "    print()\n",
        "\n",
        "    # ì•„ì›ƒë¼ì´ë„ˆ í† í”½ì˜ í‚¤ì›Œë“œ í™•ì¸\n",
        "    print(\"ì•„ì›ƒë¼ì´ë„ˆ í† í”½(-1)ì˜ ëŒ€í‘œ í‚¤ì›Œë“œ:\")\n",
        "    try:\n",
        "        outlier_words = topic_model.get_topic(-1)\n",
        "        if outlier_words:\n",
        "            # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ì¶œë ¥\n",
        "            for word, score in outlier_words[:10]:\n",
        "                print(f\"  - {word} ({score:.4f})\")\n",
        "        else:\n",
        "            print(\"  í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    except:\n",
        "        print(\"  ì•„ì›ƒë¼ì´ë„ˆ í† í”½ì˜ í‚¤ì›Œë“œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
        "    print()\n",
        "\n",
        "    # í† í”½ë³„ ë¬¸ì„œ ë¶„í¬ (ì•„ì›ƒë¼ì´ë„ˆ í¬í•¨)\n",
        "    print(\"í† í”½ë³„ ë¬¸ì„œ ë¶„í¬:\")\n",
        "    result_df = topic_info[['Topic', 'Count', 'Representation']].copy()\n",
        "    result_df['              ë¹„ìœ¨(%)'] = (result_df['Count'] / total_docs * 100).round(2)\n",
        "    print(result_df)\n",
        "\n",
        "    return valid_topics, noise_count"
      ],
      "metadata": {
        "id": "Q-eCaYdHsu7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_topics, noise_count = analyze_topic_distribution(topic_model, topics)"
      ],
      "metadata": {
        "id": "SV49W2NGswRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì‹œê°í™” ì£¼ë§ ì œì™¸ ì¼ ë‹¨ìœ„ : ì‹¤íŒ¨"
      ],
      "metadata": {
        "id": "rqo5Ux5fdOqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_topics_over_time(df, topic_model, topics):\n",
        "    # ë‚ ì§œ ë°ì´í„° ì¤€ë¹„\n",
        "    timestamps = df['inp_date'].tolist()\n",
        "\n",
        "    # ì‹œê³„ì—´ í† í”½ ì‹œê°í™”\n",
        "    topics_over_time = topic_model.topics_over_time(\n",
        "        df['preprocessed_content'].tolist(),\n",
        "        timestamps,\n",
        "        topics=topics,\n",
        "        nr_bins=59,  # í‰ì¼ ê¸°ê°„ì— ë§ì¶¤\n",
        "        datetime_format=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    # ì¼ ë‹¨ìœ„ ë¶„ì„ ì¦ëª…ì„ ìœ„í•œ ì¶œë ¥\n",
        "    print(\"== ì‹œê³„ì—´ ë¶„ì„ ì •ë³´ ==\")\n",
        "    print(f\"ì‹œê³„ì—´ ë°ì´í„° í˜•íƒœ: {topics_over_time.shape}\")\n",
        "    print(f\"ì‹œê³„ì—´ ë°ì´í„° ì»¬ëŸ¼: {topics_over_time.columns.tolist()}\")\n",
        "    print(\"\\n== ë‚ ì§œ ìƒ˜í”Œ (ì²˜ìŒ 5ê°œ) ==\")\n",
        "    unique_timestamps = topics_over_time['Timestamp'].unique()\n",
        "    print(f\"ì´ íƒ€ì„ìŠ¤íƒ¬í”„ ìˆ˜: {len(unique_timestamps)}\")\n",
        "    print(unique_timestamps[:5])\n",
        "\n",
        "    # ì‹œê³„ì—´ ì‹œê°í™” - íˆ´íŒì— ë‚ ì§œ í¬ë§· ì¶”ê°€\n",
        "    fig = topic_model.visualize_topics_over_time(\n",
        "        topics_over_time,\n",
        "        top_n_topics=10,\n",
        "        width=1200,\n",
        "        height=600\n",
        "    )\n",
        "\n",
        "    # íˆ´íŒ ë‚ ì§œ í˜•ì‹ ìˆ˜ì •\n",
        "    fig.update_traces(\n",
        "        hovertemplate='<b>í† í”½: %{customdata}</b><br>ë‚ ì§œ: %{x|%Y-%m-%d}<br>ë¹ˆë„: %{y:.2f}<extra></extra>'\n",
        "    )\n",
        "\n",
        "    # Xì¶• ë‚ ì§œ í˜•ì‹ ìˆ˜ì •\n",
        "    fig.update_xaxes(\n",
        "        tickformat=\"%Y-%m-%d\",\n",
        "        title=\"ë‚ ì§œ\"\n",
        "    )\n",
        "\n",
        "    # Yì¶• ë ˆì´ë¸” ìˆ˜ì •\n",
        "    fig.update_yaxes(\n",
        "        title=\"í† í”½ ë¹ˆë„\"\n",
        "    )\n",
        "\n",
        "    # ì œëª© ì¶”ê°€\n",
        "    fig.update_layout(\n",
        "        title=\"ì‹œê°„ì— ë”°ë¥¸ í† í”½ ë³€í™”\",\n",
        "        legend_title=\"í† í”½\"\n",
        "    )\n",
        "\n",
        "    return fig, topics_over_time"
      ],
      "metadata": {
        "id": "Dav_969yXZJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 82ì¼ ì„¤ì •\n",
        "fig, topics_over_time = visualize_topics_over_time(df, topic_model, topics)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "XOyJEZj-aGxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\n",
        "\n",
        "ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì£¼ë§ì„ ì œì™¸í–ˆëŠ”ë°ë„ topic_model.topics_over_timeì˜ ì‹œê°í™”ì—ì„œ ì£¼ë§ì´ í¬í•¨ë˜ëŠ” ê²ƒì€ ì´ í•¨ìˆ˜ê°€ ë°ì´í„°ì˜ ì‹œê°„ ë²”ìœ„ë¥¼ ê· ë“±í•˜ê²Œ ë‚˜ëˆ„ëŠ” ë°©ì‹ ë•Œë¬¸ì…ë‹ˆë‹¤.\n",
        "topic_model.topics_over_time í•¨ìˆ˜ëŠ” ì‹œì‘ ë‚ ì§œì™€ ë ë‚ ì§œ ì‚¬ì´ë¥¼ nr_bins ê°œìˆ˜ë§Œí¼ ê· ë“±í•˜ê²Œ ë‚˜ëˆ„ì–´ ì‹œê°„ êµ¬ê°„ì„ ìƒì„±í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ì£¼ë§/í‰ì¼ êµ¬ë¶„ì€ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì „ì²´ ê¸°ê°„(1ì›” 12ì¼~4ì›” 3ì¼)ì„ nr_bins=59ê°œë¡œ ë‚˜ëˆ„ë©´, ê° êµ¬ê°„ì—ëŠ” ì£¼ë§ì´ í¬í•¨ëœ ë‚ ì§œê°€ í‘œì‹œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "topic_model.topics_over_timeì€ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê±¸ë¡œ ê²°ë¡ "
      ],
      "metadata": {
        "id": "Suh5TaD6nfEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R18yyXAadED6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ì¼ë‹¨ìœ„+ì£¼ë‹¨ìœ„"
      ],
      "metadata": {
        "id": "k0y1X7p2cxP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_weekly_topic_distribution(df, topics, topic_model):\n",
        "    \"\"\"ì£¼ ë‹¨ìœ„ í† í”½ ë¶„í¬ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜\"\"\"\n",
        "    # í† í”½ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    topic_df = topic_model.get_topic_info()\n",
        "\n",
        "    # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "    topic_df['Representation'] = topic_df['Representation'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "    # í† í”½ IDì™€ í‚¤ì›Œë“œë¥¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ì—°ê²°\n",
        "    df_with_topics = df.copy()\n",
        "    df_with_topics['topic_id'] = topics\n",
        "    df_with_topics['topic_keywords'] = df_with_topics['topic_id'].map(\n",
        "        dict(zip(topic_df['Topic'], topic_df['Representation']))\n",
        "    )\n",
        "\n",
        "    # ì£¼ ë‹¨ìœ„ ì‹œê°„ ìƒì„± (ì‹œì‘ì¼ì€ ì›”ìš”ì¼)\n",
        "    df_with_topics['time_unit'] = df_with_topics['inp_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "\n",
        "    # ì£¼ ë‹¨ìœ„ë¡œ í† í”½ ì¹´ìš´íŠ¸ ì§‘ê³„\n",
        "    topic_keyword_distribution = df_with_topics.groupby(['time_unit', 'topic_keywords']).size().unstack(fill_value=0)\n",
        "\n",
        "    # ì´ ì£¼ ìˆ˜ ë° ì£¼ ì‹œì‘ì¼ ì¶œë ¥\n",
        "    num_weeks = len(topic_keyword_distribution.index)\n",
        "    print(f\"ì´ ì£¼ ìˆ˜: {num_weeks}ì£¼\")\n",
        "    print(\"ì£¼ ì‹œì‘ì¼ ëª©ë¡:\")\n",
        "    for week_start in topic_keyword_distribution.index:\n",
        "        print(week_start.strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "    return topic_keyword_distribution"
      ],
      "metadata": {
        "id": "qoQuKG9PZad9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_distribution = create_weekly_topic_distribution(df, topics, topic_model)"
      ],
      "metadata": {
        "id": "Y5CkK64tZsu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_topic_timeseries(df, topics, topic_model, time_unit='day', custom_data=None):\n",
        "\n",
        "    import plotly.graph_objs as go\n",
        "    import pandas as pd\n",
        "\n",
        "    # í† í”½ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "\n",
        "    # ì•„ì›ƒë¼ì´ì–´ ì œì™¸í•œ ëª¨ë“  í† í”½(-1 ì œì™¸)\n",
        "    all_topics = topic_info[topic_info['Topic'] != -1]['Topic'].tolist()\n",
        "\n",
        "    # ìƒ‰ìƒ íŒ”ë ˆíŠ¸\n",
        "    colors = [\n",
        "        '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
        "        '#aec7e8', '#ffbb78', '#98df8a', '#ff9896', '#c5b0d5',\n",
        "        '#c49c94', '#f7b6d2', '#c7c7c7', '#dbdb8d', '#9edae5',\n",
        "        '#636363', '#6baed6', '#fd8d3c', '#74c476', '#969696',\n",
        "        '#3182bd', '#e6550d', '#31a354', '#756bb1', '#de2d26'\n",
        "    ]\n",
        "\n",
        "    # Figure ìƒì„±\n",
        "    fig = go.Figure()\n",
        "\n",
        "    if time_unit == 'day':\n",
        "        # ì¼ë³„ ë¶„ì„\n",
        "        topic_df = pd.DataFrame({\n",
        "            'date': df['inp_date'].dt.date,\n",
        "            'topic': topics\n",
        "        })\n",
        "\n",
        "        # ì¼ë³„-í† í”½ë³„ ì§‘ê³„\n",
        "        daily_counts = topic_df.groupby(['date', 'topic']).size().reset_index(name='count')\n",
        "\n",
        "        # ê° í† í”½ë³„ trace ì¶”ê°€\n",
        "        for i, topic_id in enumerate(all_topics):\n",
        "            topic_data = daily_counts[daily_counts['topic'] == topic_id]\n",
        "\n",
        "            # í† í”½ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°\n",
        "            topic_words = [word for word, _ in topic_model.get_topic(topic_id)][:3]\n",
        "            topic_label = f\"í† í”½ {topic_id}: {', '.join(topic_words)}\"\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=topic_data['date'],\n",
        "                y=topic_data['count'],\n",
        "                mode='lines+markers',\n",
        "                name=topic_label,\n",
        "                line=dict(color=colors[i % len(colors)]),\n",
        "                marker=dict(size=6),\n",
        "                hovertemplate='<b>%{text}</b><br>ë‚ ì§œ: %{x|%Y-%m-%d}<br>ë¬¸ì„œ ìˆ˜: %{y}<extra></extra>',\n",
        "                text=[topic_label] * len(topic_data)\n",
        "            ))\n",
        "\n",
        "        # ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
        "        fig.update_layout(\n",
        "            title={\n",
        "                'text': 'ì‚¼ì„±ì „ì ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„<br><sup>ì´ê¸°ê°„ 2025-01-12 ~ 2025-04-03 : ì „ì²´82ì¼ - ì£¼ë§23ì¼ = í‰ì¼59ì¼ ì¼ ë‹¨ìœ„</sup>',\n",
        "                'y': 0.95,\n",
        "                'x': 0.5,\n",
        "                'xanchor': 'center',\n",
        "                'yanchor': 'top',\n",
        "                'font': {'size': 20, 'color': '#1f1f1f'}\n",
        "            },\n",
        "            xaxis=dict(title='ë‚ ì§œ', tickformat='%Y-%m-%d', gridcolor='lightgray'),\n",
        "            yaxis=dict(title='ë¬¸ì„œ ìˆ˜', gridcolor='lightgray'),\n",
        "            legend=dict(title='í† í”½', orientation='v'),\n",
        "            hovermode='closest',\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "\n",
        "    elif time_unit == 'week':\n",
        "        # ì£¼ë³„ ë¶„ì„\n",
        "        if custom_data is None:\n",
        "            # í† í”½ ì •ë³´ ê°€ì ¸ì˜¤ê¸°\n",
        "            topic_df = topic_model.get_topic_info()\n",
        "\n",
        "            # ë¦¬ìŠ¤íŠ¸ í˜•íƒœì˜ í‚¤ì›Œë“œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜\n",
        "            topic_df['Representation'] = topic_df['Representation'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "            # í† í”½ IDì™€ í‚¤ì›Œë“œë¥¼ ì›ë³¸ ë°ì´í„°í”„ë ˆì„ì— ì—°ê²°\n",
        "            df_with_topics = df.copy()\n",
        "            df_with_topics['topic_id'] = topics\n",
        "            df_with_topics['topic_keywords'] = df_with_topics['topic_id'].map(\n",
        "                dict(zip(topic_df['Topic'], topic_df['Representation']))\n",
        "            )\n",
        "\n",
        "            # ì£¼ ë‹¨ìœ„ ì‹œê°„ ìƒì„± (ì‹œì‘ì¼ì€ ì›”ìš”ì¼)\n",
        "            df_with_topics['time_unit'] = df_with_topics['inp_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "\n",
        "            # ì£¼ ë‹¨ìœ„ë¡œ í† í”½ ì¹´ìš´íŠ¸ ì§‘ê³„\n",
        "            weekly_data = df_with_topics.groupby(['time_unit', 'topic_id']).size().reset_index(name='count')\n",
        "        else:\n",
        "            # ì‚¬ì „ ì§‘ê³„ëœ ë°ì´í„° ì‚¬ìš©\n",
        "            weekly_data = custom_data.reset_index()\n",
        "\n",
        "        # ê° í† í”½ë³„ trace ì¶”ê°€\n",
        "        for i, topic_id in enumerate(all_topics):\n",
        "            topic_data = weekly_data[weekly_data['topic_id'] == topic_id] if 'topic_id' in weekly_data.columns else None\n",
        "\n",
        "            if topic_data is not None and not topic_data.empty:\n",
        "                # í† í”½ ë‹¨ì–´ ê°€ì ¸ì˜¤ê¸°\n",
        "                topic_words = [word for word, _ in topic_model.get_topic(topic_id)][:3]\n",
        "                topic_label = f\"í† í”½ {topic_id}: {', '.join(topic_words)}\"\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=topic_data['time_unit'],\n",
        "                    y=topic_data['count'],\n",
        "                    mode='lines+markers',\n",
        "                    name=topic_label,\n",
        "                    line=dict(color=colors[i % len(colors)]),\n",
        "                    marker=dict(size=8),\n",
        "                    hovertemplate='<b>%{text}</b><br>ì£¼ ì‹œì‘ì¼: %{x|%Y-%m-%d}<br>ë¬¸ì„œ ìˆ˜: %{y}<extra></extra>',\n",
        "                    text=[topic_label] * len(topic_data)\n",
        "                ))\n",
        "\n",
        "        # ë ˆì´ì•„ì›ƒ ì„¤ì •\n",
        "        fig.update_layout(\n",
        "            title={\n",
        "                'text': 'ì‚¼ì„±ì „ì ë‰´ìŠ¤ í‚¤ì›Œë“œ ë¶„ì„<br><sup>ì´ê¸°ê°„ 2025-01-12 ~ 2025-04-03 : ì´ 12ì£¼ ì£¼ë§ ì œì™¸, ì›” ~ ê¸ˆ ì£¼ë‹¨ìœ„</sup>',\n",
        "                'y': 0.95,\n",
        "                'x': 0.5,\n",
        "                'xanchor': 'center',\n",
        "                'yanchor': 'top',\n",
        "                'font': {'size': 20, 'color': '#1f1f1f'}\n",
        "            },\n",
        "            xaxis=dict(title='ì£¼ ì‹œì‘ì¼', tickformat='%Y-%m-%d', gridcolor='lightgray'),\n",
        "            yaxis=dict(title='ë¬¸ì„œ ìˆ˜', gridcolor='lightgray'),\n",
        "            legend=dict(title='í† í”½', orientation='v'),\n",
        "            hovermode='closest',\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"ì§€ì›ë˜ì§€ ì•ŠëŠ” ì‹œê°„ ë‹¨ìœ„ì…ë‹ˆë‹¤. 'day' ë˜ëŠ” 'week'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\")\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "_sec8-5qcDTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ì¼ë³„ ì‹œê³„ì—´ ê·¸ë˜í”„\n",
        "daily_fig = create_topic_timeseries(df, topics, topic_model, time_unit='day')\n",
        "daily_fig.show()\n",
        "print()\n",
        "\n",
        "# ì£¼ë³„ ì‹œê³„ì—´ ê·¸ë˜í”„\n",
        "weekly_fig = create_topic_timeseries(df, topics, topic_model, time_unit='week')\n",
        "weekly_fig.show()"
      ],
      "metadata": {
        "id": "nA6ynYyFcGmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs6rmlTSnGGA"
      },
      "source": [
        "# ë¬¸ì„œ íŠ¹ì§•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypWTpbg7kXum"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# ğŸ—“ï¸ ë‚ ì§œë§Œ ì¶”ì¶œ (ì‹œê°„ ì œê±°)\n",
        "df['date_only'] = df['inp_date'].dt.date\n",
        "\n",
        "# ğŸ“Š ë‚ ì§œë³„ ë¬¸ì„œ ìˆ˜ ì§‘ê³„\n",
        "daily_counts = df.groupby('date_only').size().reset_index(name='ë¬¸ì„œìˆ˜')\n",
        "\n",
        "# ğŸ“ˆ ì„  ê·¸ë˜í”„ ì‹œê°í™”\n",
        "fig = px.line(daily_counts, x='date_only', y='ë¬¸ì„œìˆ˜',\n",
        "              title='ğŸ“… ë‚ ì§œë³„ ë¬¸ì„œ ìˆ˜',\n",
        "              labels={'date_only': 'ë‚ ì§œ', 'ë¬¸ì„œìˆ˜': 'ë¬¸ì„œ ìˆ˜'},\n",
        "              markers=True)\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='ë‚ ì§œ',\n",
        "    yaxis_title='ë¬¸ì„œ ìˆ˜',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# â±ï¸ ê¸°ì¤€ ì‹œì‘ì¼ (ë°ì´í„° ì¤‘ ê°€ì¥ ì˜¤ë˜ëœ ë‚ ì§œ)\n",
        "start_date = df['inp_date'].min().normalize()\n",
        "\n",
        "# âŒ› ë©°ì¹  ì§€ë‚¬ëŠ”ì§€ ê³„ì‚° â†’ 7ì¼ ë‹¨ìœ„ë¡œ ê·¸ë£¹ ë²ˆí˜¸ ì§€ì •\n",
        "df['custom_week_group'] = ((df['inp_date'] - start_date).dt.days // 7)\n",
        "\n",
        "# ğŸ“… ê·¸ë£¹ì˜ ì‹œì‘ ë‚ ì§œ ì»¬ëŸ¼ ìƒì„± (optional)\n",
        "df['custom_week_start'] = df['custom_week_group'].apply(lambda x: start_date + pd.Timedelta(days=7 * x))\n",
        "\n",
        "# ğŸ“Š ê·¸ë£¹ë³„ ë¬¸ì„œ ìˆ˜ ì§‘ê³„\n",
        "weekly_custom = df.groupby('custom_week_start').size().reset_index(name='ë¬¸ì„œìˆ˜')\n",
        "\n",
        "# ğŸ“ˆ ì‹œê°í™”\n",
        "fig = px.line(weekly_custom, x='custom_week_start', y='ë¬¸ì„œìˆ˜',\n",
        "              title='ğŸ—“ï¸ ì‚¬ìš©ì ì •ì˜ ê¸°ì¤€ ì£¼ê°„ ë¬¸ì„œ ìˆ˜ (ì²˜ìŒ ë‚ ì§œ ê¸°ì¤€)',\n",
        "              labels={'custom_week_start': 'ì£¼ ì‹œì‘ì¼', 'ë¬¸ì„œìˆ˜': 'ë¬¸ì„œ ìˆ˜'},\n",
        "              markers=True)\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='ì£¼ì°¨ (ì‚¬ìš©ì ê¸°ì¤€)',\n",
        "    yaxis_title='ë¬¸ì„œ ìˆ˜',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrnLyF3JDJlF"
      },
      "outputs": [],
      "source": [
        "barchart = topic_model.visualize_barchart()\n",
        "barchart.show()\n",
        "print('\\n')\n",
        "\n",
        "heatmap = topic_model.visualize_heatmap()\n",
        "heatmap.show()\n",
        "print('\\n')\n",
        "\n",
        "hierarchy = topic_model.visualize_hierarchy()\n",
        "hierarchy.show()\n",
        "print('\\n')\n",
        "\n",
        "term_rank = topic_model.visualize_term_rank()\n",
        "term_rank.show()\n",
        "print('\\n')\n",
        "\n",
        "topics = topic_model.visualize_topics()\n",
        "topics.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sLpgAZhneHF"
      },
      "source": [
        "### Q) ìƒìŠ¹í•˜ê³  ìˆë‹¤ê³  í•´ë„ ë‹¤ë¥¸ í† í”½ì˜ ìµœí•˜ì ì—ë„ ëª»ë¯¸ì¹˜ëŠ” ê²½ìš°ì—ëŠ” ì •ë§ ìƒìŠ¹í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆì„ê²ƒì¸ê°€?\n",
        "\n",
        "âœ… ë°©ë²• 1: ìƒëŒ€ ìƒìŠ¹ë¥  + ì ˆëŒ€ê°’ ì¡°ê±´ í•¨ê»˜ ì‚¬ìš©\n",
        "growth_df = growth_df[(growth_df['Recent Avg'] > 10)]  # ì˜ˆ: ìµœê·¼ í‰ê·  ë¬¸ì„œ ìˆ˜ 10 ì´ìƒ\n",
        "â†’ ì¦‰, ìƒìŠ¹í•œ ê±´ ë§ì§€ë§Œ ì–´ëŠ ì •ë„ ê·œëª¨ëŠ” ìˆì–´ì•¼ ì§„ì§œ ê¸‰ìƒìŠ¹ìœ¼ë¡œ ì¸ì •\n",
        "\n",
        "âœ… ë°©ë²• 2: Z-score ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€\n",
        "ì „ì²´ í† í”½ì˜ ìµœê·¼ í‰ê·  ë¶„í¬ë¥¼ ë³´ê³ ,\n",
        "\n",
        "í‰ê·  ëŒ€ë¹„ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚œ ì •ë„ì¸ì§€(í‘œì¤€í¸ì°¨ ê¸°ì¤€)ë¥¼ ë³´ì •í•˜ëŠ” ë°©ì‹\n",
        "from scipy.stats import zscore\n",
        "\n",
        "growth_df['Z-score'] = zscore(growth_df['Increase'])\n",
        "growth_df = growth_df[growth_df['Z-score'] > 1.5]\n",
        "\n",
        "âœ… ë°©ë²• 3: \"ìƒëŒ€ìˆœìœ„ ë³€í™”\" ê¸°ë°˜ ì ‘ê·¼\n",
        "ì´ì „ ì£¼ì°¨ì— ë¹„í•´ í˜„ì¬ ì „ì²´ í† í”½ ì¤‘ ìˆœìœ„ê°€ ì–¼ë§ˆë‚˜ ì˜¬ëëŠ”ê°€ë¥¼ ë³´ëŠ” ë°©ë²•\n",
        "\n",
        "ì˜ˆ:\n",
        "\n",
        "ê°¤ëŸ­ì‹œê°€ ì „ì²´ ìˆœìœ„ 23ìœ„ â†’ ìµœê·¼ ì£¼ 5ìœ„ â†’ 18ë‹¨ê³„ ìƒìŠ¹\n",
        "â†’ ì´ëŸ° ë°©ì‹ì€ ì‘ì€ í† í”½ë„ ê³ ë ¤í•˜ë˜, ë¬´ì‹œí•  ìˆ˜ì¤€ì˜ ì‘ìŒì€ ì œì™¸ ê°€ëŠ¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGb0whM8kSZF"
      },
      "outputs": [],
      "source": [
        "#ìˆ˜ì •í•„ìš” ì‹¤í–‰ì•ˆí•´ë´„\n",
        "def get_trending_topics(df, n_weeks=3, top_k=5, min_recent_avg=10, min_increase_pct=30, sort_by='increase'):\n",
        "    \"\"\"\n",
        "    ìµœê·¼ nì£¼ ë™ì•ˆ ê¸‰ìƒìŠ¹í•œ í† í”½ì„ ìë™ìœ¼ë¡œ ë¦¬í¬íŠ¸í•´ì£¼ëŠ” í•¨ìˆ˜\n",
        "\n",
        "    Parameters:\n",
        "    - df: ë°ì´í„°í”„ë ˆì„ (ì£¼ ë‹¨ìœ„ ì»¬ëŸ¼ 'inp_date', 'topic_keywords' í•„ìš”)\n",
        "    - n_weeks: ìµœê·¼ ë¹„êµí•  ì£¼ ìˆ˜\n",
        "    - top_k: ë¦¬í¬íŠ¸í•  í† í”½ ìˆ˜\n",
        "    - min_recent_avg: ìµœê·¼ í‰ê·  ë¬¸ì„œ ìˆ˜ ìµœì†Œ ê¸°ì¤€\n",
        "    - min_increase_pct: ìµœì†Œ ì¦ê°€ ë¹„ìœ¨ %\n",
        "    - sort_by: ì •ë ¬ ê¸°ì¤€ ('increase' or 'increase_pct')\n",
        "\n",
        "    Returns:\n",
        "    - ìƒìŠ¹ í† í”½ ìš”ì•½ DataFrame\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    df['week'] = df['inp_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "    weekly_counts = df.groupby(['week', 'topic_keywords']).size().unstack(fill_value=0)\n",
        "\n",
        "    recent_weeks = weekly_counts.index[-n_weeks:]\n",
        "    prev_weeks = weekly_counts.index[-2*n_weeks:-n_weeks]\n",
        "\n",
        "    recent_avg = weekly_counts.loc[recent_weeks].mean()\n",
        "    prev_avg = weekly_counts.loc[prev_weeks].mean()\n",
        "\n",
        "    increase = recent_avg - prev_avg\n",
        "    increase_pct = ((increase) / (prev_avg + 1e-6)) * 100\n",
        "\n",
        "    growth_df = pd.DataFrame({\n",
        "        'Prev Avg': prev_avg,\n",
        "        'Recent Avg': recent_avg,\n",
        "        'Increase': increase,\n",
        "        'Increase (%)': increase_pct\n",
        "    })\n",
        "\n",
        "    # ì¡°ê±´ í•„í„°ë§\n",
        "    growth_df = growth_df[\n",
        "        (growth_df['Recent Avg'] >= min_recent_avg) &\n",
        "        (growth_df['Increase (%)'] >= min_increase_pct)\n",
        "    ]\n",
        "\n",
        "    # ì •ë ¬\n",
        "    if sort_by == 'increase_pct':\n",
        "        growth_df = growth_df.sort_values(by='Increase (%)', ascending=False)\n",
        "    else:\n",
        "        growth_df = growth_df.sort_values(by='Increase', ascending=False)\n",
        "\n",
        "    return growth_df.head(top_k).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOts0u-ikTjB"
      },
      "outputs": [],
      "source": [
        "trending = get_trending_topics(df,\n",
        "                                n_weeks=3,\n",
        "                                top_k=5,\n",
        "                                min_recent_avg=10,\n",
        "                                min_increase_pct=50,\n",
        "                                sort_by='increase_pct')\n",
        "\n",
        "print(\"ğŸ”¥ ìµœê·¼ 3ì£¼ê°„ ê¸‰ìƒìŠ¹í•œ í† í”½ TOP 5\")\n",
        "print(trending)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Vw22I-oTGn"
      },
      "source": [
        "### Q) ë™ì¼í•œ ë‚´ìš©ì˜ ë‰´ìŠ¤ê°€ ë™ì‹œë‹¤ë°œì ìœ¼ë¡œ ì˜¬ë ¤ì§„ ê²½ìš°(ë™ì¼í•œ ë‰´ìŠ¤ë¥¼ ë¿Œë¦°ê²½ìš°)ê°€ ì¡´ì¬í•˜ëŠ”ê°€?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZLkts5IpMYP"
      },
      "outputs": [],
      "source": [
        "# 'content' ì»¬ëŸ¼ì—ì„œ ì¤‘ë³µëœ ê°’ë“¤ë§Œ ì¶”ì¶œ (ëª¨ë“  ì¤‘ë³µ í¬í•¨)\n",
        "duplicated_rows = df[df.duplicated(subset='content', keep=False)]\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(duplicated_rows[['content']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHZfgjl6plT_"
      },
      "outputs": [],
      "source": [
        "# ì¤‘ë³µëœ contentë§Œ í•„í„°ë§ (ëª¨ë“  ì¤‘ë³µ í¬í•¨)\n",
        "duplicated_rows = df[df.duplicated(subset='content', keep=False)]\n",
        "\n",
        "# ì¤‘ë³µ ë¬¸ì„œ ì¤‘ì—ì„œ ê³ ìœ í•œ ê²ƒë§Œ í•œ ë²ˆì”©ë§Œ ë‚¨ê¸°ê¸°\n",
        "unique_duplicates = duplicated_rows.drop_duplicates(subset='content')\n",
        "\n",
        "# ê²°ê³¼ ì¶œë ¥\n",
        "print(unique_duplicates[['content']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Ys6W4EqNUZ"
      },
      "source": [
        "### Q) ë°ì´í„° ìˆ˜ì§‘ ë°©ì‹ì˜ ì°¨ì´, ê¸°ìê°€ ì‚´ì§ë§Œ ìˆ˜ì •í•œ ê²ƒìœ¼ë¡œ ì¸í•´ ê±°ì˜ ìœ ì‚¬í•¨ì—ë„ ì¤‘ë³µë°ì´í„°ë¡œ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ê²½ìš°ëŠ” ì—†ì„ê¹Œ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIa8fOb1qcTl"
      },
      "outputs": [],
      "source": [
        "## ë„ˆë¬´ ëŠë ¤ì„œ faissë¡œ ì½”ë“œ ì¬êµ¬ì¶• í•„ìš”\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# 2. ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
        "docs = df['content'].tolist()\n",
        "\n",
        "# 3. ì„ë² ë”© ìƒì„±\n",
        "embeddings = model.encode(docs, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# 4. ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
        "\n",
        "# 5. ìœ ì‚¬ë„ 0.95 ì´ìƒì¸ ë¬¸ì„œ ìŒì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ë³µ ê·¸ë£¹ ë§Œë“¤ê¸°\n",
        "threshold = 0.95\n",
        "visited = set()\n",
        "duplicate_groups = []\n",
        "\n",
        "for i in tqdm(range(len(docs))):\n",
        "    if i in visited:\n",
        "        continue\n",
        "    group = [i]\n",
        "    for j in range(i + 1, len(docs)):\n",
        "        if cosine_scores[i][j] >= threshold:\n",
        "            group.append(j)\n",
        "            visited.add(j)\n",
        "    if len(group) > 1:\n",
        "        duplicate_groups.append(group)\n",
        "        visited.update(group)\n",
        "\n",
        "# 6. ì¤‘ë³µ ê·¸ë£¹ í™•ì¸\n",
        "print(f\"ìœ ì‚¬í•œ ë¬¸ì„œ ê·¸ë£¹ ìˆ˜: {len(duplicate_groups)}\")\n",
        "\n",
        "# 7. ê° ê·¸ë£¹ì—ì„œ í•˜ë‚˜ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ë¥¼ ì œê±° ëŒ€ìƒìœ¼ë¡œ ê¸°ë¡\n",
        "to_remove = set()\n",
        "for group in duplicate_groups:\n",
        "    # group[0]ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ ì œê±°\n",
        "    to_remove.update(group[1:])\n",
        "\n",
        "# 8. ì¤‘ë³µëœ ë¬¸ì„œë§Œ ë”°ë¡œ ì¶”ì¶œ\n",
        "semantic_duplicates = df.iloc[list(to_remove)]\n",
        "\n",
        "# 9. ì¤‘ë³µì„ ì œì™¸í•œ ê³ ìœ í•œ ë¬¸ì„œë§Œ ì¶”ì¶œ\n",
        "df_semantic_dedup = df.drop(index=to_remove).reset_index(drop=True)\n",
        "\n",
        "# 10. ê²°ê³¼ ì¶œë ¥\n",
        "print(f\"ì¤‘ë³µ ë¬¸ì„œ ìˆ˜ (ìœ ì‚¬ë„ {threshold} ì´ìƒ): {len(to_remove)}\")\n",
        "print(\"âœ… ê³ ìœ í•œ ë¬¸ì„œ ìˆ˜:\", df_semantic_dedup.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8iXBw4OwFOK"
      },
      "source": [
        "Q) ë²¡í„°ë¼ì´ì € ì¢…ë¥˜ ë‹¤ë¥´ê²Œ?\n",
        "Q) UMAP ì°¨ì› ì¶•ì†Œë¥¼ ëª‡ê°œë¡œ í•´ì•¼ ì í•©í• ì§€?\n",
        "Q) HDBSCAN ìµœì†Œ í´ëŸ¬ìŠ¤í„° í¬ê¸°ë¥¼ ëª‡ê°œë¡œ í•˜ëŠ” ê²ƒì´ ì¢‹ì„ì§€?\n",
        "Q) ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ì„ ë‹¤ë¥´ê²Œ?\n",
        "Q) ì‚¬ìš©ì ì‚¬ì „ êµ¬ì¶•ì´ í•„ìš”í• ì§€? í•„ìš”í•˜ë‹¤ë©´ ì–´ë–»ê²Œ êµ¬ì¶•í• ì§€?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}