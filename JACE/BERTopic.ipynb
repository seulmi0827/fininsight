{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seulmi0827/fininsight/blob/main/JACE/BERTopic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KKg9qiHMveXi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "m39j846ZIk_l"
      },
      "outputs": [],
      "source": [
        "!pip install -q bertopic\n",
        "!pip install -q bertopic[visualization]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "2HV9vZ72ztpC"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk -y\n",
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!apt-get install curl -y\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiXsIGy7Yqyf"
      },
      "source": [
        "# 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjA6M7xyKFNo"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "def parse_datetime(date_string):\n",
        "   try:\n",
        "       return datetime.fromisoformat(date_string)\n",
        "   except ValueError:\n",
        "       print(f\"Invalid date format: {date_string}\")\n",
        "       return None\n",
        "\n",
        "\n",
        "from konlpy.tag import Mecab\n",
        "import pandas as pd\n",
        "\n",
        "mecab = Mecab()\n",
        "\n",
        "def preprocess(text):\n",
        "    pos_tagged = mecab.pos(text)\n",
        "\n",
        "    filtered = [\n",
        "        word for word, pos in pos_tagged\n",
        "        if (\n",
        "            pos.startswith('NN') or  # 명사\n",
        "            pos.startswith('VV') or  # 동사\n",
        "            pos.startswith('VA') or  # 형용사\n",
        "            pos == 'MAG'             # 일반 부사\n",
        "        ) and len(word) > 1          # 1글자 이상만\n",
        "    ]\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    # 데이터프레임 로드\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # 주말 뉴스 비율 확인 (참고용)\n",
        "    test = pd.read_csv(file_path)\n",
        "    test['inp_date'] = pd.to_datetime(test['inp_date'])\n",
        "    test['weekday'] = test['inp_date'].dt.weekday\n",
        "\n",
        "    # 날짜 범위 계산\n",
        "    min_date = test['inp_date'].min()\n",
        "    max_date = test['inp_date'].max()\n",
        "    total_days = (max_date - min_date).days + 1\n",
        "\n",
        "    # 주말 날짜 수 계산\n",
        "    date_range = pd.date_range(start=min_date, end=max_date)\n",
        "    weekend_days = sum(date.weekday() >= 5 for date in date_range)\n",
        "    weekday_days = total_days - weekend_days\n",
        "\n",
        "    print(f\"데이터 기간: {min_date} ~ {max_date}\")\n",
        "    print(f\"총 기간: {total_days}일\")\n",
        "    print(f\"평일 기간: {weekday_days}일\")\n",
        "    print(f\"주말 기간: {weekend_days}일\")\n",
        "\n",
        "    # 주말 날짜 확인\n",
        "    print(\"주말 날짜 확인 : \")\n",
        "    print(test[test['weekday'].isin([5, 6])][['inp_date', 'weekday']])\n",
        "    print()\n",
        "\n",
        "    # 통계 출력\n",
        "    total_news = len(test)\n",
        "    weekend = len(test[test['weekday'].isin([5, 6])])\n",
        "    weekday = total_news - weekend\n",
        "\n",
        "    print(f\"평일 기사 수: {weekday}개 ({weekday / total_news * 100:.2f}%)\")\n",
        "    print(f\"주말 기사 수: {weekend}개 ({weekend / total_news * 100:.2f}%)\")\n",
        "\n",
        "    # 본 데이터 전처리\n",
        "    df['inp_date'] = df['inp_date'].apply(parse_datetime)\n",
        "    df = df[~df['inp_date'].dt.dayofweek.isin([5, 6])]\n",
        "    print('주말 제거된 문서 수 : ', len(df))\n",
        "\n",
        "    # 주말 제거 후 날짜별 데이터 확인\n",
        "    df['date'] = df['inp_date'].dt.date  # 시간 정보 제외한 날짜만 추출\n",
        "    date_counts = df['date'].value_counts().sort_index()\n",
        "\n",
        "    # 전체 기간 생성\n",
        "    full_date_range = pd.date_range(start=df['inp_date'].min().date(),\n",
        "                                   end=df['inp_date'].max().date(),\n",
        "                                   freq='D')\n",
        "\n",
        "    # 주말 제외한 평일만 필터링\n",
        "    weekday_dates = [date for date in full_date_range if date.weekday() < 5]\n",
        "\n",
        "    # 날짜별 문서 수 통계\n",
        "    print(\"\\n== 날짜별 문서 수 통계 ==\")\n",
        "    print(f\"평균 문서 수: {date_counts.mean():.2f}\")\n",
        "    print(f\"최소 문서 수: {date_counts.min()} (날짜: {date_counts.idxmin()})\")\n",
        "    print(f\"최대 문서 수: {date_counts.max()} (날짜: {date_counts.idxmax()})\")\n",
        "\n",
        "    # 텍스트 전처리\n",
        "    df['content'] = df['content'].fillna('').astype(str)\n",
        "    df['preprocessed_content'] = df['content'].apply(lambda x: ' '.join(preprocess(x)))\n",
        "\n",
        "    # 토픽 모델링용 데이터 준비\n",
        "    preprocessed_content = df['content'].apply(preprocess).tolist()\n",
        "\n",
        "    return df, preprocessed_content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df, preprocessed_content = load_and_preprocess_data(\"/content/drive/MyDrive/Colab Notebooks/삼성전자_10000.csv\")"
      ],
      "metadata": {
        "id": "d2BfXQ0ofL9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYJpxaq3H4Fr"
      },
      "outputs": [],
      "source": [
        "print('새로 추가된 전처리 컬럼 확인 : ', df.columns.tolist())\n",
        "print()\n",
        "print(df[['inp_date', 'preprocessed_content']][:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQTO-lhg1hAB"
      },
      "source": [
        "# Mecab과 SBERT를 이용한 Bertopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gkrnj7A4enM"
      },
      "outputs": [],
      "source": [
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def initialize_topic_model(seed):\n",
        "    umap_model = UMAP(\n",
        "        n_neighbors=15,\n",
        "        n_components=5,\n",
        "        min_dist=0.0,\n",
        "        metric='cosine',\n",
        "        random_state=seed\n",
        "    )\n",
        "\n",
        "    hdbscan_model = HDBSCAN(\n",
        "        min_cluster_size=20,\n",
        "        metric='euclidean',\n",
        "        prediction_data=True,\n",
        "        gen_min_span_tree=True,\n",
        "        cluster_selection_method='eom'\n",
        "    )\n",
        "\n",
        "    embedding_model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "    vectorizer = CountVectorizer(stop_words=None)\n",
        "\n",
        "    return BERTopic(\n",
        "        language=\"korean\",\n",
        "        nr_topics=20, # 유효 토픽 결정\n",
        "        top_n_words=7, # 토픽 별 키워드 수 결정\n",
        "        calculate_probabilities=True,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=hdbscan_model,\n",
        "        embedding_model=embedding_model,\n",
        "        vectorizer_model=vectorizer,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "def run_topic_modeling(preprocessed_content):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(\"사용중인 디바이스 : \", device)\n",
        "    print()\n",
        "\n",
        "    # SEED = 42\n",
        "    # set_seed(SEED)\n",
        "    random_seed = random.randint(0, 10000)\n",
        "\n",
        "    topic_model = initialize_topic_model(random_seed)\n",
        "    content_for_topic = [' '.join(doc) for doc in preprocessed_content]\n",
        "    topics, probs = topic_model.fit_transform(content_for_topic)\n",
        "\n",
        "    return topics, probs, topic_model"
      ],
      "metadata": {
        "id": "VCT4N5u3VtQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics, probs, topic_model = run_topic_modeling(preprocessed_content)"
      ],
      "metadata": {
        "id": "om5rZFc0VwNQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_topic_distribution(topic_model, topics):\n",
        "    # 토픽 정보 가져오기\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "\n",
        "    # 유효 토픽과 노이즈 분리\n",
        "    valid_topics = topic_info[topic_info['Topic'] != -1]\n",
        "    noise_info = topic_info[topic_info['Topic'] == -1]\n",
        "\n",
        "    # 토픽 통계 계산\n",
        "    num_topics = valid_topics.shape[0]\n",
        "    total_docs = len(topics)\n",
        "    noise_count = noise_info['Count'].values[0] if not noise_info.empty else 0\n",
        "    valid_docs = total_docs - noise_count\n",
        "\n",
        "    # 결과 출력\n",
        "    print(f\"=== 토픽 분석 결과 ===\")\n",
        "    print(f\"유효 토픽 수: {num_topics}개\")\n",
        "    print(f\"전체 문서 수: {total_docs}개\")\n",
        "    print(f\"노이즈 문서 수: {noise_count}개 ({noise_count/total_docs:.2%})\")\n",
        "    print(f\"유효 토픽 문서 수: {valid_docs}개 ({valid_docs/total_docs:.2%})\")\n",
        "    print()\n",
        "\n",
        "    # 아웃라이너 토픽의 키워드 확인\n",
        "    print(\"아웃라이너 토픽(-1)의 대표 키워드:\")\n",
        "    try:\n",
        "        outlier_words = topic_model.get_topic(-1)\n",
        "        if outlier_words:\n",
        "            # 상위 10개 키워드 출력\n",
        "            for word, score in outlier_words[:10]:\n",
        "                print(f\"  - {word} ({score:.4f})\")\n",
        "        else:\n",
        "            print(\"  키워드가 없습니다.\")\n",
        "    except:\n",
        "        print(\"  아웃라이너 토픽의 키워드를 가져올 수 없습니다.\")\n",
        "    print()\n",
        "\n",
        "    # 토픽별 문서 분포 (아웃라이너 포함)\n",
        "    print(\"토픽별 문서 분포:\")\n",
        "    result_df = topic_info[['Topic', 'Count', 'Representation']].copy()\n",
        "    result_df['              비율(%)'] = (result_df['Count'] / total_docs * 100).round(2)\n",
        "    print(result_df)\n",
        "\n",
        "    return valid_topics, noise_count"
      ],
      "metadata": {
        "id": "Q-eCaYdHsu7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_topics, noise_count = analyze_topic_distribution(topic_model, topics)"
      ],
      "metadata": {
        "id": "SV49W2NGswRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 시각화 주말 제외 일 단위 : 실패"
      ],
      "metadata": {
        "id": "rqo5Ux5fdOqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_topics_over_time(df, topic_model, topics):\n",
        "    # 날짜 데이터 준비\n",
        "    timestamps = df['inp_date'].tolist()\n",
        "\n",
        "    # 시계열 토픽 시각화\n",
        "    topics_over_time = topic_model.topics_over_time(\n",
        "        df['preprocessed_content'].tolist(),\n",
        "        timestamps,\n",
        "        topics=topics,\n",
        "        nr_bins=59,  # 평일 기간에 맞춤\n",
        "        datetime_format=\"%Y-%m-%d %H:%M:%S\"\n",
        "    )\n",
        "\n",
        "    # 일 단위 분석 증명을 위한 출력\n",
        "    print(\"== 시계열 분석 정보 ==\")\n",
        "    print(f\"시계열 데이터 형태: {topics_over_time.shape}\")\n",
        "    print(f\"시계열 데이터 컬럼: {topics_over_time.columns.tolist()}\")\n",
        "    print(\"\\n== 날짜 샘플 (처음 5개) ==\")\n",
        "    unique_timestamps = topics_over_time['Timestamp'].unique()\n",
        "    print(f\"총 타임스탬프 수: {len(unique_timestamps)}\")\n",
        "    print(unique_timestamps[:5])\n",
        "\n",
        "    # 시계열 시각화 - 툴팁에 날짜 포맷 추가\n",
        "    fig = topic_model.visualize_topics_over_time(\n",
        "        topics_over_time,\n",
        "        top_n_topics=10,\n",
        "        width=1200,\n",
        "        height=600\n",
        "    )\n",
        "\n",
        "    # 툴팁 날짜 형식 수정\n",
        "    fig.update_traces(\n",
        "        hovertemplate='<b>토픽: %{customdata}</b><br>날짜: %{x|%Y-%m-%d}<br>빈도: %{y:.2f}<extra></extra>'\n",
        "    )\n",
        "\n",
        "    # X축 날짜 형식 수정\n",
        "    fig.update_xaxes(\n",
        "        tickformat=\"%Y-%m-%d\",\n",
        "        title=\"날짜\"\n",
        "    )\n",
        "\n",
        "    # Y축 레이블 수정\n",
        "    fig.update_yaxes(\n",
        "        title=\"토픽 빈도\"\n",
        "    )\n",
        "\n",
        "    # 제목 추가\n",
        "    fig.update_layout(\n",
        "        title=\"시간에 따른 토픽 변화\",\n",
        "        legend_title=\"토픽\"\n",
        "    )\n",
        "\n",
        "    return fig, topics_over_time"
      ],
      "metadata": {
        "id": "Dav_969yXZJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 82일 설정\n",
        "fig, topics_over_time = visualize_topics_over_time(df, topic_model, topics)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "XOyJEZj-aGxb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##\n",
        "\n",
        "전처리 과정에서 주말을 제외했는데도 topic_model.topics_over_time의 시각화에서 주말이 포함되는 것은 이 함수가 데이터의 시간 범위를 균등하게 나누는 방식 때문입니다.\n",
        "topic_model.topics_over_time 함수는 시작 날짜와 끝 날짜 사이를 nr_bins 개수만큼 균등하게 나누어 시간 구간을 생성합니다. 이 과정에서 주말/평일 구분은 하지 않습니다. 따라서 전체 기간(1월 12일~4월 3일)을 nr_bins=59개로 나누면, 각 구간에는 주말이 포함된 날짜가 표시될 수 있습니다.\n",
        "\n",
        "topic_model.topics_over_time은 사용하지 않는 걸로 결론"
      ],
      "metadata": {
        "id": "Suh5TaD6nfEW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R18yyXAadED6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 일단위+주단위"
      ],
      "metadata": {
        "id": "k0y1X7p2cxP0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_weekly_topic_distribution(df, topics, topic_model):\n",
        "    \"\"\"주 단위 토픽 분포를 계산하는 함수\"\"\"\n",
        "    # 토픽 정보 가져오기\n",
        "    topic_df = topic_model.get_topic_info()\n",
        "\n",
        "    # 리스트 형태의 키워드를 문자열로 변환\n",
        "    topic_df['Representation'] = topic_df['Representation'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "    # 토픽 ID와 키워드를 원본 데이터프레임에 연결\n",
        "    df_with_topics = df.copy()\n",
        "    df_with_topics['topic_id'] = topics\n",
        "    df_with_topics['topic_keywords'] = df_with_topics['topic_id'].map(\n",
        "        dict(zip(topic_df['Topic'], topic_df['Representation']))\n",
        "    )\n",
        "\n",
        "    # 주 단위 시간 생성 (시작일은 월요일)\n",
        "    df_with_topics['time_unit'] = df_with_topics['inp_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "\n",
        "    # 주 단위로 토픽 카운트 집계\n",
        "    topic_keyword_distribution = df_with_topics.groupby(['time_unit', 'topic_keywords']).size().unstack(fill_value=0)\n",
        "\n",
        "    # 총 주 수 및 주 시작일 출력\n",
        "    num_weeks = len(topic_keyword_distribution.index)\n",
        "    print(f\"총 주 수: {num_weeks}주\")\n",
        "    print(\"주 시작일 목록:\")\n",
        "    for week_start in topic_keyword_distribution.index:\n",
        "        print(week_start.strftime(\"%Y-%m-%d\"))\n",
        "\n",
        "    return topic_keyword_distribution"
      ],
      "metadata": {
        "id": "qoQuKG9PZad9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_distribution = create_weekly_topic_distribution(df, topics, topic_model)"
      ],
      "metadata": {
        "id": "Y5CkK64tZsu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_topic_timeseries(df, topics, topic_model, time_unit='day', custom_data=None):\n",
        "\n",
        "    import plotly.graph_objs as go\n",
        "    import pandas as pd\n",
        "\n",
        "    # 토픽 정보 가져오기\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "\n",
        "    # 아웃라이어 제외한 모든 토픽(-1 제외)\n",
        "    all_topics = topic_info[topic_info['Topic'] != -1]['Topic'].tolist()\n",
        "\n",
        "    # 색상 팔레트\n",
        "    colors = [\n",
        "        '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd',\n",
        "        '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf',\n",
        "        '#aec7e8', '#ffbb78', '#98df8a', '#ff9896', '#c5b0d5',\n",
        "        '#c49c94', '#f7b6d2', '#c7c7c7', '#dbdb8d', '#9edae5',\n",
        "        '#636363', '#6baed6', '#fd8d3c', '#74c476', '#969696',\n",
        "        '#3182bd', '#e6550d', '#31a354', '#756bb1', '#de2d26'\n",
        "    ]\n",
        "\n",
        "    # Figure 생성\n",
        "    fig = go.Figure()\n",
        "\n",
        "    if time_unit == 'day':\n",
        "        # 일별 분석\n",
        "        topic_df = pd.DataFrame({\n",
        "            'date': df['inp_date'].dt.date,\n",
        "            'topic': topics\n",
        "        })\n",
        "\n",
        "        # 일별-토픽별 집계\n",
        "        daily_counts = topic_df.groupby(['date', 'topic']).size().reset_index(name='count')\n",
        "\n",
        "        # 각 토픽별 trace 추가\n",
        "        for i, topic_id in enumerate(all_topics):\n",
        "            topic_data = daily_counts[daily_counts['topic'] == topic_id]\n",
        "\n",
        "            # 토픽 단어 가져오기\n",
        "            topic_words = [word for word, _ in topic_model.get_topic(topic_id)][:3]\n",
        "            topic_label = f\"토픽 {topic_id}: {', '.join(topic_words)}\"\n",
        "\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=topic_data['date'],\n",
        "                y=topic_data['count'],\n",
        "                mode='lines+markers',\n",
        "                name=topic_label,\n",
        "                line=dict(color=colors[i % len(colors)]),\n",
        "                marker=dict(size=6),\n",
        "                hovertemplate='<b>%{text}</b><br>날짜: %{x|%Y-%m-%d}<br>문서 수: %{y}<extra></extra>',\n",
        "                text=[topic_label] * len(topic_data)\n",
        "            ))\n",
        "\n",
        "        # 레이아웃 설정\n",
        "        fig.update_layout(\n",
        "            title={\n",
        "                'text': '삼성전자 뉴스 키워드 분석<br><sup>총기간 2025-01-12 ~ 2025-04-03 : 전체82일 - 주말23일 = 평일59일 일 단위</sup>',\n",
        "                'y': 0.95,\n",
        "                'x': 0.5,\n",
        "                'xanchor': 'center',\n",
        "                'yanchor': 'top',\n",
        "                'font': {'size': 20, 'color': '#1f1f1f'}\n",
        "            },\n",
        "            xaxis=dict(title='날짜', tickformat='%Y-%m-%d', gridcolor='lightgray'),\n",
        "            yaxis=dict(title='문서 수', gridcolor='lightgray'),\n",
        "            legend=dict(title='토픽', orientation='v'),\n",
        "            hovermode='closest',\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "\n",
        "    elif time_unit == 'week':\n",
        "        # 주별 분석\n",
        "        if custom_data is None:\n",
        "            # 토픽 정보 가져오기\n",
        "            topic_df = topic_model.get_topic_info()\n",
        "\n",
        "            # 리스트 형태의 키워드를 문자열로 변환\n",
        "            topic_df['Representation'] = topic_df['Representation'].apply(lambda x: ', '.join(x))\n",
        "\n",
        "            # 토픽 ID와 키워드를 원본 데이터프레임에 연결\n",
        "            df_with_topics = df.copy()\n",
        "            df_with_topics['topic_id'] = topics\n",
        "            df_with_topics['topic_keywords'] = df_with_topics['topic_id'].map(\n",
        "                dict(zip(topic_df['Topic'], topic_df['Representation']))\n",
        "            )\n",
        "\n",
        "            # 주 단위 시간 생성 (시작일은 월요일)\n",
        "            df_with_topics['time_unit'] = df_with_topics['inp_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "\n",
        "            # 주 단위로 토픽 카운트 집계\n",
        "            weekly_data = df_with_topics.groupby(['time_unit', 'topic_id']).size().reset_index(name='count')\n",
        "        else:\n",
        "            # 사전 집계된 데이터 사용\n",
        "            weekly_data = custom_data.reset_index()\n",
        "\n",
        "        # 각 토픽별 trace 추가\n",
        "        for i, topic_id in enumerate(all_topics):\n",
        "            topic_data = weekly_data[weekly_data['topic_id'] == topic_id] if 'topic_id' in weekly_data.columns else None\n",
        "\n",
        "            if topic_data is not None and not topic_data.empty:\n",
        "                # 토픽 단어 가져오기\n",
        "                topic_words = [word for word, _ in topic_model.get_topic(topic_id)][:3]\n",
        "                topic_label = f\"토픽 {topic_id}: {', '.join(topic_words)}\"\n",
        "\n",
        "                fig.add_trace(go.Scatter(\n",
        "                    x=topic_data['time_unit'],\n",
        "                    y=topic_data['count'],\n",
        "                    mode='lines+markers',\n",
        "                    name=topic_label,\n",
        "                    line=dict(color=colors[i % len(colors)]),\n",
        "                    marker=dict(size=8),\n",
        "                    hovertemplate='<b>%{text}</b><br>주 시작일: %{x|%Y-%m-%d}<br>문서 수: %{y}<extra></extra>',\n",
        "                    text=[topic_label] * len(topic_data)\n",
        "                ))\n",
        "\n",
        "        # 레이아웃 설정\n",
        "        fig.update_layout(\n",
        "            title={\n",
        "                'text': '삼성전자 뉴스 키워드 분석<br><sup>총기간 2025-01-12 ~ 2025-04-03 : 총 12주 주말 제외, 월 ~ 금 주단위</sup>',\n",
        "                'y': 0.95,\n",
        "                'x': 0.5,\n",
        "                'xanchor': 'center',\n",
        "                'yanchor': 'top',\n",
        "                'font': {'size': 20, 'color': '#1f1f1f'}\n",
        "            },\n",
        "            xaxis=dict(title='주 시작일', tickformat='%Y-%m-%d', gridcolor='lightgray'),\n",
        "            yaxis=dict(title='문서 수', gridcolor='lightgray'),\n",
        "            legend=dict(title='토픽', orientation='v'),\n",
        "            hovermode='closest',\n",
        "            plot_bgcolor='white'\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"지원되지 않는 시간 단위입니다. 'day' 또는 'week'를 사용하세요.\")\n",
        "\n",
        "    return fig"
      ],
      "metadata": {
        "id": "_sec8-5qcDTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 일별 시계열 그래프\n",
        "daily_fig = create_topic_timeseries(df, topics, topic_model, time_unit='day')\n",
        "daily_fig.show()\n",
        "print()\n",
        "\n",
        "# 주별 시계열 그래프\n",
        "weekly_fig = create_topic_timeseries(df, topics, topic_model, time_unit='week')\n",
        "weekly_fig.show()"
      ],
      "metadata": {
        "id": "nA6ynYyFcGmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs6rmlTSnGGA"
      },
      "source": [
        "# 문서 특징"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypWTpbg7kXum"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# 🗓️ 날짜만 추출 (시간 제거)\n",
        "df['date_only'] = df['inp_date'].dt.date\n",
        "\n",
        "# 📊 날짜별 문서 수 집계\n",
        "daily_counts = df.groupby('date_only').size().reset_index(name='문서수')\n",
        "\n",
        "# 📈 선 그래프 시각화\n",
        "fig = px.line(daily_counts, x='date_only', y='문서수',\n",
        "              title='📅 날짜별 문서 수',\n",
        "              labels={'date_only': '날짜', '문서수': '문서 수'},\n",
        "              markers=True)\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='날짜',\n",
        "    yaxis_title='문서 수',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show()\n",
        "\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# ⏱️ 기준 시작일 (데이터 중 가장 오래된 날짜)\n",
        "start_date = df['inp_date'].min().normalize()\n",
        "\n",
        "# ⌛ 며칠 지났는지 계산 → 7일 단위로 그룹 번호 지정\n",
        "df['custom_week_group'] = ((df['inp_date'] - start_date).dt.days // 7)\n",
        "\n",
        "# 📅 그룹의 시작 날짜 컬럼 생성 (optional)\n",
        "df['custom_week_start'] = df['custom_week_group'].apply(lambda x: start_date + pd.Timedelta(days=7 * x))\n",
        "\n",
        "# 📊 그룹별 문서 수 집계\n",
        "weekly_custom = df.groupby('custom_week_start').size().reset_index(name='문서수')\n",
        "\n",
        "# 📈 시각화\n",
        "fig = px.line(weekly_custom, x='custom_week_start', y='문서수',\n",
        "              title='🗓️ 사용자 정의 기준 주간 문서 수 (처음 날짜 기준)',\n",
        "              labels={'custom_week_start': '주 시작일', '문서수': '문서 수'},\n",
        "              markers=True)\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='주차 (사용자 기준)',\n",
        "    yaxis_title='문서 수',\n",
        "    hovermode='x unified'\n",
        ")\n",
        "\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrnLyF3JDJlF"
      },
      "outputs": [],
      "source": [
        "barchart = topic_model.visualize_barchart()\n",
        "barchart.show()\n",
        "print('\\n')\n",
        "\n",
        "heatmap = topic_model.visualize_heatmap()\n",
        "heatmap.show()\n",
        "print('\\n')\n",
        "\n",
        "hierarchy = topic_model.visualize_hierarchy()\n",
        "hierarchy.show()\n",
        "print('\\n')\n",
        "\n",
        "term_rank = topic_model.visualize_term_rank()\n",
        "term_rank.show()\n",
        "print('\\n')\n",
        "\n",
        "topics = topic_model.visualize_topics()\n",
        "topics.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sLpgAZhneHF"
      },
      "source": [
        "### Q) 상승하고 있다고 해도 다른 토픽의 최하점에도 못미치는 경우에는 정말 상승한다고 볼 수 있을것인가?\n",
        "\n",
        "✅ 방법 1: 상대 상승률 + 절대값 조건 함께 사용\n",
        "growth_df = growth_df[(growth_df['Recent Avg'] > 10)]  # 예: 최근 평균 문서 수 10 이상\n",
        "→ 즉, 상승한 건 맞지만 어느 정도 규모는 있어야 진짜 급상승으로 인정\n",
        "\n",
        "✅ 방법 2: Z-score 기반 이상치 탐지\n",
        "전체 토픽의 최근 평균 분포를 보고,\n",
        "\n",
        "평균 대비 얼마나 벗어난 정도인지(표준편차 기준)를 보정하는 방식\n",
        "from scipy.stats import zscore\n",
        "\n",
        "growth_df['Z-score'] = zscore(growth_df['Increase'])\n",
        "growth_df = growth_df[growth_df['Z-score'] > 1.5]\n",
        "\n",
        "✅ 방법 3: \"상대순위 변화\" 기반 접근\n",
        "이전 주차에 비해 현재 전체 토픽 중 순위가 얼마나 올랐는가를 보는 방법\n",
        "\n",
        "예:\n",
        "\n",
        "갤럭시가 전체 순위 23위 → 최근 주 5위 → 18단계 상승\n",
        "→ 이런 방식은 작은 토픽도 고려하되, 무시할 수준의 작음은 제외 가능"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGb0whM8kSZF"
      },
      "outputs": [],
      "source": [
        "#수정필요 실행안해봄\n",
        "def get_trending_topics(df, n_weeks=3, top_k=5, min_recent_avg=10, min_increase_pct=30, sort_by='increase'):\n",
        "    \"\"\"\n",
        "    최근 n주 동안 급상승한 토픽을 자동으로 리포트해주는 함수\n",
        "\n",
        "    Parameters:\n",
        "    - df: 데이터프레임 (주 단위 컬럼 'inp_date', 'topic_keywords' 필요)\n",
        "    - n_weeks: 최근 비교할 주 수\n",
        "    - top_k: 리포트할 토픽 수\n",
        "    - min_recent_avg: 최근 평균 문서 수 최소 기준\n",
        "    - min_increase_pct: 최소 증가 비율 %\n",
        "    - sort_by: 정렬 기준 ('increase' or 'increase_pct')\n",
        "\n",
        "    Returns:\n",
        "    - 상승 토픽 요약 DataFrame\n",
        "    \"\"\"\n",
        "    import pandas as pd\n",
        "\n",
        "    df['week'] = df['inp_date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
        "    weekly_counts = df.groupby(['week', 'topic_keywords']).size().unstack(fill_value=0)\n",
        "\n",
        "    recent_weeks = weekly_counts.index[-n_weeks:]\n",
        "    prev_weeks = weekly_counts.index[-2*n_weeks:-n_weeks]\n",
        "\n",
        "    recent_avg = weekly_counts.loc[recent_weeks].mean()\n",
        "    prev_avg = weekly_counts.loc[prev_weeks].mean()\n",
        "\n",
        "    increase = recent_avg - prev_avg\n",
        "    increase_pct = ((increase) / (prev_avg + 1e-6)) * 100\n",
        "\n",
        "    growth_df = pd.DataFrame({\n",
        "        'Prev Avg': prev_avg,\n",
        "        'Recent Avg': recent_avg,\n",
        "        'Increase': increase,\n",
        "        'Increase (%)': increase_pct\n",
        "    })\n",
        "\n",
        "    # 조건 필터링\n",
        "    growth_df = growth_df[\n",
        "        (growth_df['Recent Avg'] >= min_recent_avg) &\n",
        "        (growth_df['Increase (%)'] >= min_increase_pct)\n",
        "    ]\n",
        "\n",
        "    # 정렬\n",
        "    if sort_by == 'increase_pct':\n",
        "        growth_df = growth_df.sort_values(by='Increase (%)', ascending=False)\n",
        "    else:\n",
        "        growth_df = growth_df.sort_values(by='Increase', ascending=False)\n",
        "\n",
        "    return growth_df.head(top_k).round(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOts0u-ikTjB"
      },
      "outputs": [],
      "source": [
        "trending = get_trending_topics(df,\n",
        "                                n_weeks=3,\n",
        "                                top_k=5,\n",
        "                                min_recent_avg=10,\n",
        "                                min_increase_pct=50,\n",
        "                                sort_by='increase_pct')\n",
        "\n",
        "print(\"🔥 최근 3주간 급상승한 토픽 TOP 5\")\n",
        "print(trending)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2Vw22I-oTGn"
      },
      "source": [
        "### Q) 동일한 내용의 뉴스가 동시다발적으로 올려진 경우(동일한 뉴스를 뿌린경우)가 존재하는가?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZLkts5IpMYP"
      },
      "outputs": [],
      "source": [
        "# 'content' 컬럼에서 중복된 값들만 추출 (모든 중복 포함)\n",
        "duplicated_rows = df[df.duplicated(subset='content', keep=False)]\n",
        "\n",
        "# 결과 출력\n",
        "print(duplicated_rows[['content']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHZfgjl6plT_"
      },
      "outputs": [],
      "source": [
        "# 중복된 content만 필터링 (모든 중복 포함)\n",
        "duplicated_rows = df[df.duplicated(subset='content', keep=False)]\n",
        "\n",
        "# 중복 문서 중에서 고유한 것만 한 번씩만 남기기\n",
        "unique_duplicates = duplicated_rows.drop_duplicates(subset='content')\n",
        "\n",
        "# 결과 출력\n",
        "print(unique_duplicates[['content']])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4Ys6W4EqNUZ"
      },
      "source": [
        "### Q) 데이터 수집 방식의 차이, 기자가 살짝만 수정한 것으로 인해 거의 유사함에도 중복데이터로 처리되지 않은 경우는 없을까?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIa8fOb1qcTl"
      },
      "outputs": [],
      "source": [
        "## 너무 느려서 faiss로 코드 재구축 필요\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# 1. 임베딩 모델 준비\n",
        "model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "\n",
        "# 2. 문서 리스트\n",
        "docs = df['content'].tolist()\n",
        "\n",
        "# 3. 임베딩 생성\n",
        "embeddings = model.encode(docs, convert_to_tensor=True, show_progress_bar=True)\n",
        "\n",
        "# 4. 유사도 행렬 계산\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
        "\n",
        "# 5. 유사도 0.95 이상인 문서 쌍을 기반으로 중복 그룹 만들기\n",
        "threshold = 0.95\n",
        "visited = set()\n",
        "duplicate_groups = []\n",
        "\n",
        "for i in tqdm(range(len(docs))):\n",
        "    if i in visited:\n",
        "        continue\n",
        "    group = [i]\n",
        "    for j in range(i + 1, len(docs)):\n",
        "        if cosine_scores[i][j] >= threshold:\n",
        "            group.append(j)\n",
        "            visited.add(j)\n",
        "    if len(group) > 1:\n",
        "        duplicate_groups.append(group)\n",
        "        visited.update(group)\n",
        "\n",
        "# 6. 중복 그룹 확인\n",
        "print(f\"유사한 문서 그룹 수: {len(duplicate_groups)}\")\n",
        "\n",
        "# 7. 각 그룹에서 하나만 남기고 나머지를 제거 대상으로 기록\n",
        "to_remove = set()\n",
        "for group in duplicate_groups:\n",
        "    # group[0]만 남기고 나머지 제거\n",
        "    to_remove.update(group[1:])\n",
        "\n",
        "# 8. 중복된 문서만 따로 추출\n",
        "semantic_duplicates = df.iloc[list(to_remove)]\n",
        "\n",
        "# 9. 중복을 제외한 고유한 문서만 추출\n",
        "df_semantic_dedup = df.drop(index=to_remove).reset_index(drop=True)\n",
        "\n",
        "# 10. 결과 출력\n",
        "print(f\"중복 문서 수 (유사도 {threshold} 이상): {len(to_remove)}\")\n",
        "print(\"✅ 고유한 문서 수:\", df_semantic_dedup.shape[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8iXBw4OwFOK"
      },
      "source": [
        "Q) 벡터라이저 종류 다르게?\n",
        "Q) UMAP 차원 축소를 몇개로 해야 적합할지?\n",
        "Q) HDBSCAN 최소 클러스터 크기를 몇개로 하는 것이 좋을지?\n",
        "Q) 문장 임베딩 모델을 다르게?\n",
        "Q) 사용자 사전 구축이 필요할지? 필요하다면 어떻게 구축할지?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}