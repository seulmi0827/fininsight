{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seulmi0827/fininsight/blob/main/JACE/full_refine_sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_ME022qAbmf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ln95Khm0fkrw"
      },
      "outputs": [],
      "source": [
        "!pip install -q kss flashtext\n",
        "!pip install -q pandarallel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XDwxn4tP60s7"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install g++ openjdk-8-jdk -y\n",
        "!pip install konlpy\n",
        "!pip install mecab-python\n",
        "!apt-get install curl -y\n",
        "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# import"
      ],
      "metadata": {
        "id": "iC3aqBEWl5ik"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9trurtnXbB1l",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "import pandas as pd\n",
        "import re\n",
        "from kss import split_sentences\n",
        "from flashtext import KeywordProcessor\n",
        "from pandarallel import pandarallel\n",
        "from konlpy.tag import Mecab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 전처리 함수"
      ],
      "metadata": {
        "id": "gmgZonMrl8bM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jg6S16jGwN0Z"
      },
      "outputs": [],
      "source": [
        "def expand_aspect_terms(aspect_terms: List[str]) -> List[str]:\n",
        "    expanded = set()\n",
        "\n",
        "    for term in aspect_terms:\n",
        "        if '_' in term:\n",
        "            clean_term = term.replace('_', ' ')\n",
        "            expanded.add(clean_term) # 언더바를 띄어쓰기로 대체한 단어 추가\n",
        "            expanded.add(clean_term.replace(\" \", \"\"))  # 띄어쓰기를 제거한 단어 추가\n",
        "        elif ' ' in term:\n",
        "            expanded.add(term) # 띄어쓰기 된 원본 단어 추가\n",
        "            clean_term = term.replace(' ', '')\n",
        "            expanded.add(clean_term) # 띄어쓰기 제거된 단어 추가\n",
        "        else:\n",
        "            clean_term = term # 언더바도 아니고 띄어쓰기도 아닌 원래 단어\n",
        "            expanded.add(clean_term)\n",
        "\n",
        "    return list(expanded)\n",
        "\n",
        "\n",
        "def extract_aspect_sentences(df, aspect_terms):\n",
        "    pandarallel.initialize(progress_bar=True, verbose=1)\n",
        "\n",
        "    expanded_terms = expand_aspect_terms(aspect_terms)\n",
        "\n",
        "    keyword_processor = KeywordProcessor()\n",
        "    for term in expanded_terms:\n",
        "        keyword_processor.add_keyword(term)\n",
        "\n",
        "    # 정규식 패턴 생성\n",
        "    pattern_dict = {\n",
        "        term: re.compile(rf'\\b{re.escape(term)}\\b') for term in expanded_terms\n",
        "    }\n",
        "\n",
        "    def process_content(content):\n",
        "        results = []\n",
        "        try:\n",
        "            # 괄호 사이에 있는 내용 제거\n",
        "            def remove_brackets(text):\n",
        "                prev_text = \"\"\n",
        "                current_text = text\n",
        "\n",
        "                while prev_text != current_text:\n",
        "                    prev_text = current_text\n",
        "                    current_text = re.sub(r'\\([^()]*\\)', '', current_text)  # 소괄호\n",
        "                    current_text = re.sub(r'\\[[^\\[\\]]*\\]', '', current_text)  # 대괄호\n",
        "                    current_text = re.sub(r'\\{[^\\{\\}]*\\}', '', current_text)  # 중괄호\n",
        "                    current_text = re.sub(r'\\<[^\\<\\>]*\\>', '', current_text)  # 꺾쇠괄호\n",
        "\n",
        "                return current_text\n",
        "\n",
        "            # 줄바꿈 기준으로 문장 분리 후 각 줄의 끝에 마침표 확인, 없으면 추가\n",
        "            lines = content.split('\\n')\n",
        "            for i in range(len(lines)):\n",
        "                lines[i] = lines[i].strip()\n",
        "                if lines[i] and not lines[i].endswith('.') and not lines[i].endswith('!') and not lines[i].endswith('?'):\n",
        "                    lines[i] = lines[i] + '.'\n",
        "\n",
        "\n",
        "            # 줄바꿈 처리된 문장들을 다시 합침\n",
        "            content = ' '.join(lines).replace('\\r', ' ')\n",
        "\n",
        "            # 괄호 사이에 있는 내용 제거\n",
        "            content = remove_brackets(content)\n",
        "\n",
        "            # 이메일 형식 제거\n",
        "            content = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}', '', content)\n",
        "\n",
        "            # 전화번호 형태 제거\n",
        "            content = re.sub(r'(\\d{2,3}[-\\s]?\\d{3,4}[-\\s]?\\d{4})', '', content)\n",
        "\n",
        "            # 특수문자(물음표) : 의문문 또는 특수기호가 물음표로 잘 못 들어간 경우\n",
        "            content = re.sub(r'\\S\\?\\S', '', content)\n",
        "\n",
        "            # 특수문자와 한글/영숫자/공백/일부 기호를 제외한 모든 문자 제거 (한자도 제거)\n",
        "            content = re.sub(r'[^0-9a-zA-Z\\s가-힣.,!%\\'\\\"]', '', content)\n",
        "\n",
        "            # 연속 공백 처리\n",
        "            content = re.sub(r'\\s+', ' ', content).strip()\n",
        "\n",
        "            # KSS로 문장 분리\n",
        "            # 문장 분리 후에는 아예 제거하는 방식으로 접근 (문장 분리 전에는 일부분만 제거하는 방식)\n",
        "            sentences = split_sentences(content)\n",
        "\n",
        "            for sentence in sentences:\n",
        "                # '..' 또는 ',,.' 등이 포함된 문장 제거\n",
        "                if '..' in sentence or '...' in sentence or ' ..' in sentence or ' ...' in sentence:\n",
        "                    continue\n",
        "\n",
        "                # 인터뷰 제거\n",
        "                if \"Q.\" in sentence:\n",
        "                    continue\n",
        "\n",
        "                # \"기자\" 패턴 체크 및 처리\n",
        "                if ' 기자 ' in sentence:\n",
        "                    # print(f\"기자 패턴 발견: {sentence}\")\n",
        "                    reporter_idx = sentence.find(' 기자 ')\n",
        "                    # print(f\"기자 인덱스: {reporter_idx}\")\n",
        "\n",
        "                    # \"기자\" 앞에 2-3글자 이름이 있는지 확인\n",
        "                    if reporter_idx >= 3:  # 최소 3글자(공백1+이름2) 이상 앞에 있어야 함\n",
        "                        prev_char = sentence[reporter_idx-1] # 동\n",
        "                        prev_two_chars = sentence[reporter_idx-2:reporter_idx] # 길동\n",
        "                        prev_three_chars = sentence[reporter_idx-3:reporter_idx] # 홍길동\n",
        "                        # print(f\"이전 1글자: '{prev_char}', 이전 2글자: '{prev_two_chars}', 이전 3글자: '{prev_three_chars}'\")\n",
        "\n",
        "                        # 2글자(외자) 또는 3글자(이름) 패턴 확인\n",
        "                        is_two_char_match = len(prev_two_chars) == 2 and re.match(r'^[가-힣]{2}$', prev_two_chars)\n",
        "                        is_three_char_match = len(prev_three_chars) == 3 and re.match(r'^[가-힣]{3}$', prev_three_chars)\n",
        "                        # print(f\"2글자 매치: {is_two_char_match}, 3글자 매치: {is_three_char_match}\")\n",
        "\n",
        "                        if is_two_char_match or is_three_char_match:\n",
        "                            # \"기자 \" 위치 이후 문장만 사용\n",
        "                            parts = sentence.split(' 기자 ', 1)  # 최대 1번만 분할\n",
        "                            # print(f\"분할 결과: {parts}\")\n",
        "                            if len(parts) > 1:\n",
        "                                old_sentence = sentence\n",
        "                                sentence = parts[1].strip()\n",
        "                                # print(f\"변경 전: '{old_sentence}'\")\n",
        "                                # print(f\"변경 후: '{sentence}'\")\n",
        "\n",
        "                                # 처리 후 문장이 비어있으면 건너뛰기\n",
        "                                if not sentence:\n",
        "                                    # print(\"비어있는 문장으로 건너뛰기\")\n",
        "                                    continue\n",
        "\n",
        "                # 30자 이하 문장 제거\n",
        "                if len(sentence) <= 30:\n",
        "                    continue\n",
        "\n",
        "                # 형태소 분석\n",
        "                mecab = Mecab()\n",
        "                pos_tagged = mecab.pos(sentence)\n",
        "\n",
        "                if not pos_tagged:\n",
        "                    continue\n",
        "\n",
        "                # 자연스럽지 않은 문장 끝 필터링\n",
        "                if (pos_tagged[-1][1].startswith('N') or   # 명사 품사\n",
        "                    # 예: NNG(일반 명사): 학교, 사과, 의자\n",
        "                    # 예: NNP(고유 명사): 서울, 한국, 태평양\n",
        "                    # 예: NNB(의존 명사): 것, 데, 수, 바\n",
        "                    pos_tagged[-1][1].startswith('J') or   # 조사 품사\n",
        "                    # 예: JKS(주격 조사): ~이, ~가\n",
        "                    # 예: JKO(목적격 조사): ~을, ~를\n",
        "                    # 예: JKB(부사격 조사): ~에, ~에서\n",
        "                    # 예: JKG(관형격 조사): ~의\n",
        "                    pos_tagged[-1][1].startswith('MM') or  # 관형사 품사\n",
        "                    # 예: MMD(지시 관형사): 이, 그, 저\n",
        "                    # 예: MMN(수 관형사): 한, 두, 세\n",
        "                    # 예: MMA(성상 관형사): 새, 헌, 첫\n",
        "                    pos_tagged[-1][1].startswith('MA') or  # 부사 품사\n",
        "                    # 예: MAG(일반 부사): 매우, 빨리, 천천히\n",
        "                    # 예: MAJ(접속 부사): 그리고, 또한, 그러나\n",
        "                    pos_tagged[-1][1] == 'SN' or          # 숫자 품사\n",
        "                    # 예: SN(숫자): 1, 2, 3, 하나, 둘, 셋\n",
        "                    (pos_tagged[-1][1] == 'EF')           # 종결 어미 품사\n",
        "                    # 예: EF(종결 어미): ~다, ~요, ~네, ~죠, ~습니다\n",
        "                    ):\n",
        "                    continue\n",
        "\n",
        "                # 공백+마침표 패턴 제거\n",
        "                sentence = re.sub(r' \\.', '.', sentence)\n",
        "\n",
        "                # 문장 복사 (이후 조사 제거를 위해)\n",
        "                processed_sentence = sentence\n",
        "\n",
        "                # 조사 제거\n",
        "                current_pos = len(processed_sentence)\n",
        "                for i in range(len(pos_tagged)-1, -1, -1):\n",
        "                    word, pos = pos_tagged[i]\n",
        "                    if pos.startswith(\"J\"):\n",
        "                        word_pos = processed_sentence.rfind(word, 0, current_pos)\n",
        "                        if word_pos != -1:\n",
        "                            processed_sentence = processed_sentence[:word_pos] + processed_sentence[word_pos+len(word):]\n",
        "                            current_pos = word_pos\n",
        "\n",
        "                # 키워드 검색\n",
        "                keywords_found = keyword_processor.extract_keywords(processed_sentence)\n",
        "                if keywords_found:\n",
        "                    for term, pattern in pattern_dict.items():\n",
        "                        if pattern.search(processed_sentence):\n",
        "                            results.append({'original_sentence': sentence, 'processed_sentence': processed_sentence, 'term': term})\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"오류 발생: {e}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    # 병렬 처리\n",
        "    test_df = df.head(2000)\n",
        "    all_results = test_df['content'].parallel_apply(process_content)\n",
        "\n",
        "    # 결과 합치기\n",
        "    flattened_results = [item for sublist in all_results for item in sublist]\n",
        "    return pd.DataFrame(flattened_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## term 통합"
      ],
      "metadata": {
        "id": "LdFJukGAmRN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# term 가져오기\n",
        "result_file1 = pd.read_csv(\"/content/drive/MyDrive/fin/키워드/사회_both_1.csv\")\n",
        "result_file2 = pd.read_csv(\"/content/drive/MyDrive/fin/키워드/사회_disagreement_re_gpt.csv\")\n",
        "merged_result = pd.concat([result_file1, result_file2], ignore_index=True)\n",
        "merged_result = merged_result.rename(columns={'단어': 'term'})\n",
        "result_terms = set(merged_result['term'].dropna().astype(str).tolist())\n",
        "\n",
        "print(len(result_file1))\n",
        "print(len(result_file2))\n",
        "print(len(merged_result))\n",
        "print(len(result_terms))\n"
      ],
      "metadata": {
        "id": "2mjtJ2ImmQnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기사 데이터 불러오기\n",
        "df_file1 = pd.read_csv('/content/drive/MyDrive/fin/뉴스 데이터/NewsResult_20250421-20250422.csv', encoding='utf-8')\n",
        "df_file2 = pd.read_csv('/content/drive/MyDrive/fin/뉴스 데이터/NewsResult_20250122-20250123.csv', encoding='utf-8')\n",
        "df_file3 = pd.read_csv('/content/drive/MyDrive/fin/뉴스 데이터/NewsResult_20241022-20241023.csv', encoding='utf-8')\n",
        "df_file4 = pd.read_csv('/content/drive/MyDrive/fin/뉴스 데이터/NewsResult_20240722-20240723.csv', encoding='utf-8')\n",
        "df_file5 = pd.read_csv('/content/drive/MyDrive/fin/뉴스 데이터/NewsResult_20240422-20240423.csv', encoding='utf-8')\n",
        "\n",
        "merged_df = pd.concat([df_file1, df_file2, df_file3, df_file4, df_file5], ignore_index=True)\n",
        "merged_df = merged_df.rename(columns={'본문': 'content'})\n",
        "df = merged_df.copy()\n",
        "df[\"content\"] = df[\"content\"].fillna(\"\").astype(str)"
      ],
      "metadata": {
        "id": "hwc2Pn2muy-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"로드된 컬럼 : {list(df.columns)}\")\n",
        "print(f\"로드된 기사: {len(df)}개\")\n",
        "print()\n",
        "\n",
        "print(\"문장 추출 시작...\")\n",
        "result_df = extract_aspect_sentences(df, result_terms)\n",
        "print(f\"매칭된 문장 : {len(result_df)}\")\n",
        "result_df[['original_sentence','term']]"
      ],
      "metadata": {
        "id": "frVp0tRcXeFi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## term별 문장수"
      ],
      "metadata": {
        "id": "pQCvktYMmGPQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "term_counts = result_df.groupby('term').size().reset_index(name='sentence_count')\n",
        "term_counts = term_counts.sort_values('sentence_count', ascending=False) # 정렬\n",
        "print(f\"토탈 카운트 : {len(term_counts)}\")\n",
        "print()\n",
        "print(term_counts)"
      ],
      "metadata": {
        "id": "FdJRwmFDVKlH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 저장"
      ],
      "metadata": {
        "id": "FM8Opwo3VL5O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7U7RmGvZnB0s"
      },
      "outputs": [],
      "source": [
        "# term 별 문장 5개 랜덤 추출\n",
        "# random_sentences = result_df.groupby('term').apply(lambda x: x.sample(min(len(x), 5))).reset_index(drop=True)\n",
        "\n",
        "# 문장 길이 컬럼 추가 후 긴 문장 5개 추출\n",
        "random_sentences = result_df.groupby('term').apply(\n",
        "    lambda x: x.assign(length=x['original_sentence'].str.len()).nlargest(min(len(x), 5), 'length')\n",
        ").reset_index(drop=True)\n",
        "\n",
        "# 필요한 컬럼만 선택 (전처리 문장(조사제거X)과 키워드)\n",
        "result_sentences = random_sentences[['original_sentence', 'term']]\n",
        "\n",
        "result_sentences.to_csv('/content/drive/MyDrive/fin/aspect_sentences/0423_final_aspect_sentences.csv', index=False, encoding='utf-8-sig')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}